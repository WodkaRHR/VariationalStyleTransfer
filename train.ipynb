{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import data, model, loss, function\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torch.utils.tensorboard as tb\n",
    "import torchvision\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_writer = tb.SummaryWriter('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_PORTION = 0.1\n",
    "ITERATIONS = 15002\n",
    "VAL_ITERATIONS = 5\n",
    "RESOLUTION = 64\n",
    "CHANNELS = 3\n",
    "STYLE_DIM = 512\n",
    "#CONTENT_DIM = 512\n",
    "BATCH_SIZE = 8\n",
    "LOSS_TYPE = 'l2'\n",
    "#CONTENT_LOSS_WEIGHTS = {'input' : 0.0, 'relu3' : 0.0, 'relu4' : 1e2, 'relu5' : 0.0}\n",
    "#STYLE_LOSS_WEIGHTS = {'input' : 0.0, 'relu1' : 1e2, 'relu2' : 1e2, 'relu3' : 1e2, 'relu4' : 1e2, 'relu5' : 1e2}\n",
    "\n",
    "CONTENT_LOSS_WEIGHTS = {\n",
    "    #'relu_1_1' : 1e-2,\n",
    "    #'relu_4_2' : 5e-3,\n",
    "    'relu_4_2' : 1e-2,\n",
    "}\n",
    "\n",
    "STYLE_LOSS_WEIGHTS = {\n",
    "    'relu_1_1' : 1e3,\n",
    "    'relu_2_1' : 1e3,\n",
    "    'relu_3_1' : 1e3,\n",
    "    'relu_4_1' : 1e3,\n",
    "    'relu_5_1' : 1e3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(content, style, content_encoder, style_encoder, decoder):\n",
    "    \"\"\" Forwards a batch through the pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    content : torch.Tensor, shape [B, C, H, W]\n",
    "        The content image.\n",
    "    style : torch.Tensor, shape [B, C, H', W']\n",
    "        The style image, usually H' = H and W' = W.\n",
    "    content_encoder : torch.nn.modules.Module\n",
    "        Encoder for content images.\n",
    "    style_encoder : torch.nn.modules.Module\n",
    "        Encoder for style images.\n",
    "    decoder : torch.nn.modules.Module\n",
    "        Decoder that uses AdaIn to decode the content and apply the style.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    reco : torch.Tensor, shape [B, C, H, W]\n",
    "        A reconstruction of the content after application of the style.\n",
    "    content_representation : torch.Tensor, shape [B, D]\n",
    "        Content representation.\n",
    "    style_representation : torch.Tensor, shape [B, D']\n",
    "        The latent style representation.\n",
    "    \"\"\"\n",
    "    content_representation = content_encoder(content)\n",
    "    style_representation = style_encoder(style)\n",
    "    #transformed_content = function.adain(content_representation, style_representation)\n",
    "    #print(content_representation.shape)\n",
    "    #print(style_representation.shape)\n",
    "    reco = decoder(content_representation, style_representation)\n",
    "    #reco = decoder(content_representation)\n",
    "    return reco, content_representation, style_representation\n",
    "\n",
    "def print_content_feature_contrib(feature_activations_x, feature_activations_y):\n",
    "    for key in feature_activations_x:\n",
    "        feature_loss = torch.nn.functional.mse_loss(feature_activations_x[key], feature_activations_y[key])\n",
    "        weight = 0.0\n",
    "        if key in CONTENT_LOSS_WEIGHTS:\n",
    "            weight = CONTENT_LOSS_WEIGHTS[key]\n",
    "        weighted_loss = weight * feature_loss\n",
    "        print(\"Content layer {} loss: {} weight: {} weighted loss: {}\".format(key, feature_loss, weight, weighted_loss))\n",
    "\n",
    "def print_style_feature_contrib(features_style, features_transformed):\n",
    "    for key, weight in STYLE_LOSS_WEIGHTS.items():\n",
    "        Gx = function.gram_matrix(features_style[key])\n",
    "        Gy = function.gram_matrix(features_transformed[key])\n",
    "        value = torch.nn.functional.mse_loss(Gx, Gy)\n",
    "        weighted_value = weight * value\n",
    "        print(f'Style loss {key} with weight {weight}: {value} weighted loss: {weighted_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_style = data.load_debug_dataset('../dataset/debug/style', resolution=RESOLUTION)\n",
    "data_content = data.load_debug_dataset('../dataset/debug/content', resolution=RESOLUTION)\n",
    "data_content_train, data_content_val = torch.utils.data.random_split(data_content, [len(data_content) - int(VAL_PORTION * len(data_content)), int(VAL_PORTION * len(data_content))])\n",
    "data_style_train, data_style_val = torch.utils.data.random_split(data_style, [len(data_style) - int(VAL_PORTION * len(data_style)), int(VAL_PORTION * len(data_style))])\n",
    "data_loader_content_train = DataLoader(data_content_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "data_loader_content_val = DataLoader(data_content_val, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "data_loader_style_train = DataLoader(data_style_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "data_loader_style_val = DataLoader(data_style_val, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "data_loader_train = data.DatasetPairIterator(data_loader_content_train, data_loader_style_train)\n",
    "data_loader_val = data.DatasetPairIterator(data_loader_content_val, data_loader_style_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING_PORTION_STYLE=100\n",
    "#data_style = data.load_debug_dataset('../dataset/style',resolution=RESOLUTION)\n",
    "data_style = data.load_debug_dataset('../dataset/debug/style', resolution=RESOLUTION)\n",
    "data_style, _ = torch.utils.data.random_split(data_style, [TRAINING_PORTION_STYLE, len(data_style) - TRAINING_PORTION_STYLE])\n",
    "data_style_train, data_style_val = torch.utils.data.random_split(data_style, [len(data_style) - int(VAL_PORTION * len(data_style)), int(VAL_PORTION * len(data_style))])\n",
    "data_loader_style_train = DataLoader(data_style_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "#data_loader_style_val = DataLoader(data_style_val, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "TRAINING_PORTION=100\n",
    "data_content = data.load_debug_dataset('../dataset/content', resolution=RESOLUTION)\n",
    "data_content, _ = torch.utils.data.random_split(data_content, [TRAINING_PORTION, len(data_content) - TRAINING_PORTION])\n",
    "data_content_train, data_content_val = torch.utils.data.random_split(data_content, [len(data_content) - int(VAL_PORTION * len(data_content)), int(VAL_PORTION * len(data_content))])\n",
    "data_loader_content_train = DataLoader(data_content_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "data_content_val = data.load_debug_dataset('../dataset/debug/content', resolution=RESOLUTION)\n",
    "data_loader_content_val = DataLoader(data_content_val, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "data_loader_train = data.DatasetPairIterator(data_loader_content_train, data_loader_style_train)\n",
    "#data_loader_val = data.DatasetPairIterator(data_loader_content_val, data_loader_style_val)\n",
    "\n",
    "# NO REAL VALIDATION, USES TRAINING STYLE DATA\n",
    "data_loader_val = data.DatasetPairIterator(data_loader_content_val, data_loader_style_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#content_encoder = model.ResNetEncoder(CONTENT_DIM, architecture=torchvision.models.resnet34)\n",
    "#style_encoder = model.ResNetEncoder(STYLE_DIM, architecture=torchvision.models.resnet18)\n",
    "#decoder = model.Decoder(CONTENT_DIM, STYLE_DIM, (RESOLUTION, RESOLUTION))\n",
    "\n",
    "# FLATTEN\n",
    "#content_encoder = model.VGGEncoder((CHANNELS, RESOLUTION, RESOLUTION), flattened_output_dim=CONTENT_DIM)\n",
    "#style_encoder = model.VGGEncoder((CHANNELS, RESOLUTION, RESOLUTION), flattened_output_dim=STYLE_DIM)\n",
    "#decoder = model.VGGDecoder((CHANNELS, RESOLUTION, RESOLUTION), STYLE_DIM, CONTENT_DIM)\n",
    "\n",
    "# NO FLATTEN\n",
    "#content_encoder = model.VGGEncoder((CHANNELS, RESOLUTION, RESOLUTION))\n",
    "#style_encoder = model.VGGEncoder((CHANNELS, RESOLUTION, RESOLUTION))\n",
    "#decoder = model.VGGDecoder((CHANNELS, RESOLUTION, RESOLUTION), STYLE_DIM, None)\n",
    "\n",
    "# ADAIN DECODER, content NO FLATTEN, style FLATTEN\n",
    "#content_encoder = model.VGGEncoder((CHANNELS, RESOLUTION, RESOLUTION))\n",
    "#style_encoder = model.VGGEncoder((CHANNELS, RESOLUTION, RESOLUTION), flattened_output_dim=STYLE_DIM)\n",
    "#decoder = model.VGGDecoder((CHANNELS, RESOLUTION, RESOLUTION), STYLE_DIM, None)\n",
    "\n",
    "# NEW ENCODER DECODER ARCHITECTURE, RESIDUAL, ADAIN\n",
    "#normalization = ['adain', 'adain', 'adain', None, None]\n",
    "#content_encoder = model.Encoder(CONTENT_DIM, num_down_convolutions=5)\n",
    "#style_encoder = model.Encoder(STYLE_DIM, num_down_convolutions=5)\n",
    "#decoder = model.Decoder(CONTENT_DIM, STYLE_DIM, RESOLUTION, normalization='in', num_adain_convolutions=3, num_up_convolutions=5)\n",
    "\n",
    "# NEW ENCODER DECODER ARCHITECTURE, RESIDUAL, ADAIN, SPATIAL CONTENT EMBEDDING\n",
    "\n",
    "CONTENT_DOWN_CONVOLUTIONS = 4\n",
    "STYLE_DOWN_CONVOLUTIONS = 4\n",
    "\n",
    "normalization = [None, None, None, None, None]\n",
    "content_encoder = model.Encoder(None, num_down_convolutions=CONTENT_DOWN_CONVOLUTIONS)\n",
    "style_encoder = model.Encoder(STYLE_DIM, num_down_convolutions=CONTENT_DOWN_CONVOLUTIONS)\n",
    "decoder = model.Decoder(None, STYLE_DIM, RESOLUTION, normalization=normalization, num_adain_convolutions=5, num_up_convolutions=CONTENT_DOWN_CONVOLUTIONS)\n",
    "\n",
    "\n",
    "loss_net = loss.LossNet()\n",
    "_ = loss_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): \n",
    "    content_encoder = content_encoder.cuda()\n",
    "    style_encoder = style_encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "    loss_net = loss_net.cuda()\n",
    "\n",
    "trainable_parameters = []\n",
    "for parameter in content_encoder.parameters():\n",
    "    trainable_parameters.append(parameter)\n",
    "for parameter in style_encoder.parameters():\n",
    "    trainable_parameters.append(parameter)\n",
    "for parameter in decoder.parameters():\n",
    "    trainable_parameters.append(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(trainable_parameters, lr=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "for (content_image, content_path), (style_image, style_path) in data_loader_train:\n",
    "    content_image, style_image = content_image[:1], style_image[:1]\n",
    "    break\n",
    "content_encoder.train()\n",
    "style_encoder.train()\n",
    "decoder.train()\n",
    "if torch.cuda.is_available():\n",
    "    content_image = content_image.to('cuda')\n",
    "    style_image = style_image.to('cuda')\n",
    "for iteration in range(ITERATIONS):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    stylized, content_encoding, style_encoding = forward(content_image, style_image, content_encoder, style_encoder, decoder)\n",
    "    \n",
    "    features_content = loss_net(content_image)\n",
    "    features_style = loss_net(style_image)\n",
    "    features_stylized = loss_net(stylized)\n",
    "    \n",
    "    perceptual_loss = loss.perceptual_loss(features_content, features_stylized, CONTENT_LOSS_WEIGHTS)\n",
    "    style_loss = loss.style_loss(features_style, features_stylized, STYLE_LOSS_WEIGHTS)\n",
    "    total_loss = perceptual_loss + style_loss\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    tb_writer.add_scalar('train loss', total_loss.item(), iteration)\n",
    "    tb_writer.add_scalar('train perceptual loss', perceptual_loss.item(), iteration)\n",
    "    tb_writer.add_scalar('train style loss', style_loss.item(), iteration)\n",
    "    print(f'\\r{iteration:5d} / {ITERATIONS}: loss : {total_loss.item():.4f} -- perceptual loss : {perceptual_loss.item():.4f} -- style loss : {style_loss.item():.4f}', end='\\r')\n",
    "    \n",
    "    if iteration % 100 == 0:\n",
    "        tb_writer.add_images('images', torch.from_numpy(np.concatenate([\n",
    "            data.vgg_normalization_undo(img.detach().cpu().numpy()) for img in [content_image, style_image, stylized]\n",
    "        ])), iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 / 15002: loss : 2.3431 -- perceptual loss : 0.2833 -- style loss : 2.0598\n",
      "Validation...\n",
      "    5 / 5: loss : 1.3779 -- perceptual loss : 0.2754 -- style loss : 1.1026\r"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3X+UVOWd5/H3d2h7cmJAiaCS7sYWmxC60SjToNmoO+aXpM22449RzA/NogeZgR2zzrpD1jmcM8wk4uTEUQeUkLibTSbSyTA/uo/aRDF6MrIKNII/KENAQOlejWh2jRrXDu13/6gCqruqu29X36p7697P65w6VNV9+t6nvjz3e5/nuVX3mrsjIiLJ8ntRV0BERMKn5C4ikkBK7iIiCaTkLiKSQEruIiIJpOQuIpJASu4iIgmk5C4ikkBK7iIiCVQT1YanTJnijY2NUW2+YrZv3/66u08NUlYxKS4NcVFMitP+UyhoTCJL7o2NjfT09ES1+Yoxs5eCllVMiktDXBST4rT/FAoaE03LiIgkkJK7iEgCKbmLiCRQZHPuYWlc/mDgsgdWXVLGmsRfsVgpJumOSdo//1hUW6zUcxcRSSAldxGRBFJyFxFJICV3EZEEqvoTqiISruG+pBDnk4dSSD13EZEEUs9dRGSIsXzFOq4C9dzNbIGZ7TazvWa2vMjyJWb2nJntNLMnzKw5/KqKiEhQoyZ3M5sArAE+DzQD1xRJ3ve7+5nufjbwt8AdoddUREQCCzItMx/Y6+77AMysA7gUyBwp4O6/ySt/POBhVlLGLgnDyqTYuHEjN910EwMDA9xwww0sXz548Lt27VqAZjPbCbwNLHb3TJFViQQWZFqmDjiY97o3994gZrbUzF4k23P/s2IrMrPFZtZjZj2HDh0qpb4iVWVgYIClS5fS3d1NJpNh/fr1ZDKD8/YXv/hFgIxGvhKm0L4t4+5r3P0M4C+AvxymzDp3b3X31qlTA9+XQKRqbd26laamJmbMmEFtbS0LFy6ks7NzUJlJkyblv0zFyHfjxo3MmjWLpqYmVq1aVbB87dq1nHnmmZAd0eg8XgmCJPc+oCHvdX3uveF0AH80nkpVAzXO6tO4/MGCR7n19fXR0HBs96mvr6evr+juMzUtI9+go5nnnnsOstO/Gs2UIEhy3wbMNLPTzawWWAh05Rcws5l5Ly8B9oRXxfhR45QyOJSWka9GM5UxanJ398PAMuCnwAvAT9x9l5mtNLP2XLFlZrYrd0LoZuC6stU4BtQ4i9NoplBdXR0HDx47ZdXb20tdXcEpq3yJH/kGHc2sWbMGYA4jjGZkeIHm3N39IXf/qLuf4e7fyL23wt27cs9vcvcWdz/b3S9y913lrHTU1DgLaTRT3Lx589izZw/79++nv7+fjo4O2tvbB5XZs2fQQDfxI9+gli5dCvA8I4xmkjJVVQ66/EAZpalxajRTXE1NDatXr+biiy9m9uzZXHXVVbS0tLBixQq6urKzm6tXrwZoScvIN8zRTFKmqspBlx8oQYmN895iC9x9HbAOoLW1tWqTXbHRzJYtWwrKDRnNfKpS9YtSW1sbbW1tg95buXLl0ed33XUXd9999y53b6103aKQP5qpq6ujo6OD+++/f1CZPXv2MHPm0VN5Gs2UQD33EmioXbogoxlIzohGCgUdzbS0tED2V/GJH82Ug3ruJchvnAMDAyxatOho42xtbaW9vZ3Vq1ezadMmSEnjDHM0A8kZ0UhxQUYzAGaWcfeLKlq5hFByL5Ea52AaaovEi6ZlJBQaaovEi3ruEhqNZkTiQz13EZEEUnIXEUkgJXcRkQRSchcRSSAldxGRBErVt2WKXb/7wKpLIqiJiEh5pSq5SyEd8ESSScldRAJRR6BQnGOiOXcRkQRSz11kiOHurRqXHplIEOq5i4gkUKDkbmYLzGy3me01s+VFlt9sZhkze9bMHjWz08KvqoiIBDVqcjezCcAa4PNkr+Z3TZEbG+8AWt39LGAD2bvsiIhIRIL03OcDe919n7v3k73JwqX5Bdz9MXf/be7lU0B9uNUUqV4bN25k1qxZNDU1sWrVqoLld9xxB2TvoaqRr4QmSHKvAw7mve7NvTec64Hu8VRKJCkGBgZYunQp3d3dZDIZ1q9fTyaTGVTmnHPOAXghTSPfIAe85uZmgGYd8EoT6glVM/sy0Ap8a5jlibkvphqnBLF161aampqYMWMGtbW1LFy4kM7OzkFlLrroIoD3cy8TP/INesDr6ekByJCSA17YgiT3PqAh73V97r1BzOwzwK1Au7u/V2xF7r7O3VvdvXXq1Kml1DcW1DiL0wGvUF9fHw0Nx3af+vp6+voKdp98w458k9I5CnrA++AHP3jkZeIPeOUQJLlvA2aa2elmVgssBLryC5jZOcB3yCb218KvZryocRbSAW/8Rhv5JqVzFOYBT4Y3anJ398PAMuCnwAvAT9x9l5mtNLP2XLFvAR8C/tHMdppZ1zCrSwQ1zkI64BVXV1fHwYPHTln19vZSV1f0lNVERhn5ptSHSclUb9gC/ULV3R8CHhry3oq8558JuV5JcqRx/vtiC81sMbAYYPr06RWsVriKHfC2bNky0p8k/oAHMG/ePPbs2cP+/fupq6ujo6OD+++/f1CZHTt2AJwGnJmGkW/QA96mTZsApgHnjTTVC6wDaG1t9XLUt1rpF6olKKFxJv48xBiN2BuD5PTIampqWL16NRdffDGzZ8/mqquuoqWlhRUrVtDVlR3g3nLLLQATSMnIN/+A19/fT0dHB+3t7YPK7NixgxtvvBGyX8NO/AGvHHRtmRIE7Y2lqXGG2RuDZPXI2traaGtrG/TeypUrjz7ftGkTZvaMu7dWum5RyD/gDQwMsGjRoqMHvNbWVtrb27nlllt4++23Ac4ws53Ay+7ePsqqJY+SewnUOAvpgCdjEeSAB2BmmbQc9MKm5F4iNc7BdMATiRcldwmNDngi8aHkLpISw12nXsIVl/sB6NsyIiIJlPqee5zvgRiVuPQ8RKR0qU/uIkGpI1BIMYmvqkrumjMUEQlGc+4iIgmk5C4ikkBK7iIiCaTkLiKSQFV1QlVEJExJ/pKGeu4iIgmknrskUpJ7ZCJBKLlXOSUxESlG0zIiIgkUKLmb2QIz221me81seZHlF5rZ02Z22MyuDL+aIiIyFqMmdzObAKwBPg80A9eYWfOQYi8DXwXuR0QG2bhxI7NmzaKpqYlVq1YVLP/5z38OMDtNnaMgMZk7dy7AH6QlJmEL0nOfT/a2aPvcvR/oAC7NL+DuB9z9WeD9MtQxltQ4CykmhQYGBli6dCnd3d1kMhnWr19PJpMZVGb69OkAB0hJ5yhoTL7//e8DvBFFHZMgSHKvAw7mve7NvZdaapyFFJPitm7dSlNTEzNmzKC2tpaFCxfS2dk5qExjYyPAu6SkcxQ0JmeddVZENUyGip5QNbPFZtZjZj2HDh2q5KZDpcZZSDEprq+vj4aGhqOv6+vr6evrK2ldSdl/woyJDC9Icu8DGvJe1+feGzN3X+fure7eOnXq1FJWEQvaYQtphy2/pOw/YUrK/lMOQZL7NmCmmZ1uZrXAQqCrvNVKD+2wxSVlp62rq+PgwWOzmr29vdTVpXpWM9SYaP8Z3qjJ3d0PA8uAnwIvAD9x911mttLM2gHMbJ6Z9QJ/DHzHzHaVs9JR0w5bKOyYJGWnnTdvHnv27GH//v309/fT0dFBe3t71NWKlGJSGYHm3N39IXf/qLuf4e7fyL23wt27cs+3uXu9ux/v7ie5e0s5Kx01Nc5CiklxNTU1rF69mosvvpjZs2dz1VVX0dLSwooVK+jqyg6At23bBnAWKekcBY1JfX09wGRSEJNy0OUHSpDfOAcGBli0aNHRxtna2kp7ezvbtm3jsssug2ON86+SfNBTTIbX1tZGW1vboPdWrlx59Pm8efMAnnX31srWLDpBYtLb24uZ7UhTXMKk5F4iNc5CiolIfOjaMiIiCaTkLiKSQJqWkcCKXV74wKpLIqiJiIxGyV1EpAIq3TlSci9iuBtgqJcqQ2k0I3GlOXcRkQRSz11EQqXRTDwouYskkO6tK7FN7mqcIiKli21yl0I64BVSTCSotLUVnVAVEUkgJXcRkQRSchcRSSAldxGRBFJyFxFJIH1bRqTKpe1bIBJMLJK7GqeIhEk5JWByN7MFwF3ABOB77r5qyPLfB34A/AHwBnC1ux8It6rRy28w7+7bzq8fXcdpkz/ADTfcwPLlyweVfe+997j22msB5pjZFsYYk2ppnIpJoaH1fHffdk585kcMDAwMGxdghpntJaH7T7GYlLOtVItyXqph1Dl3M5sArAE+DzQD15hZ85Bi1wP/x92bgL8Dbg+ldjHl7w/w60fu5eQ//isymQzr168nk8kMKnPfffcxefJkgOdRTID0xQSOxaW7u3vEuACHtf8ck8a2ErYgJ1TnA3vdfZ+79wMdwKVDylwK/M/c8w3Ap83MwqtmvPS/8ktqTpzGcSeeykdXPMLLJ57NJ//kdhqXP3j0SNzZ2cl111135E8UE9IXEzgWl0+te2HEuJDtsUMK4qK2MrIjcch/lMLcfeQCZlcCC9z9htzrrwDnuvuyvDLP58r05l6/mCvz+pB1LQYW517OAnaPsOkpwOsjLA+qHOuZDEwCXsq9/jDwIeDlvPItwC+Bj7j71JBiMrQe4xH2ekKLCaSyrfzW3U+CUPafOMcDott/wvo8Ya6rlPWc5u5TRy3l7iM+gCvJzrMfef0VYPWQMs8D9XmvXwSmjLbuUbbbM56/L+d6oopJnOOimMSnrcQ5HlG2lbA+TxxjXOwRZFqmD2jIe12fe69oGTOrAU7g2DAziRSTQopJcYpLIcWkAoIk923ATDM73cxqgYVA15AyXcCRCbIrgZ957rCUUIpJIcWkOMWlkGJSCQGHDm1k579eBG7NvbcSaM89/wDwj8BeYCswI4ThyuKQhj1lWU8UMYl7XBSTeLSVuMcjqrYS1ueJY4yLPUY9oSoiItVH15YREUkgJXcRkQSKTXI3s2+Z2S/M7Fkz+xczO3GYcgfM7Dkz22lmPXnvLzCz3Wa218yWF/m73zezH+eWbzGzxiJlGszsMTPLmNkuM7upSJk/NLM3c9vfaWYrxvfJR6a4FFJMCrajeBSR+riUazK/hBMLnwNqcs9vB24fptwBhnzflew1b14EZgC1wDNA85AyfwqszT1fCPy4yLqnAXNzzyeSPeEzdD1/CDyguEQXF8VE8VBcRn/Epufu7g+7++Hcy6fIfvc1qFAukeDur7j707nnbwEvAHVj+yThUlwKKSaDKR7FpT0usUnuQywCuodZ5sDDZrbdsj89hmywDuaV6aUwgEfL5P7D3wROGq4CuSHWOcCWIos/YWbPmFm3mbWM/FFCpbgUUkwGUzyKS11cKno9dzPbBJxaZNGt7t6ZK3MrcBj40TCrOd/d+8zsZOARM/tFGer5IeCfgK+5+2+GLH6a7LUd3jazNuBfgZnj3J7iUrgtxWTwdhSP4ttTXIZR0eTu7p8ZabmZfRX4AvBpz01GFVlHX+7f18zsX8gOnzYT/OfMvTbCz5nN7Diy/wk/cvd/LrL93+Q9f8jM7jGzKV7k4ldBKS5FP49iMnhdikfxz6S4DCesyfvxPoAFQAaYOkKZ44GJec//V+7vaoB9wOkcO/nRMuRvlzL45MdPiqzfyN505M4R6nAqx66mOZ/slexMcalcXBQTxUNxCfD5yxXYEv4j9pKdv9qZexwJ2keAh3LPZ+SC/Aywi9zPlnPLxv1zZuB8svNvz+bVow1YAizJlVmW2/YzZE/S/DvFpbJxUUwUD8Vl9IcuPyAikkBx/baMiIiMg5K7iEgCKbmLiCRQRb8KmW/KlCne2NgY1eYrZvv27a97kPsdopgMJw1xUUyK0/5TKGhMIkvujY2N9PT0jF6wypnZS6OXylJMiktDXBST4rT/FAoaE03LiIgkkJK7iEgCKbmLiCRQZHPuYWlc/mDR9w+suqTCNYm/YrFKe5zSHpO0f/7hJCEu6rmLiCSQkruISAIpuZdg9+7dnH322UcfkyZN4s477xxU5vHHH+eEE04AaA71vogxpZiIxEvVz7lHYdasWezcuROAgYEB6urquOyyywrKXXDBBTz44IMZd2+tdB0rTTFJDp3HSgb13Mfp0Ucf5YwzzuC0006LuiqxoZiUZIKZbTCzX5jZC2b2iagrJNVNyX2cOjo6uOaaa4oue/LJJyE7BTHsfRHNbLGZ9ZhZz6FDh8pY08oZb0wgmXEZRQOw0d0/Bnyc7I2URUqmaZlx6O/vp6uri9tuu61g2dy5c3nppZeYOHFiBvh7hrkvoruvA9YBtLa2Vv3F9cOICZQnLsNNN0TtzTffBJgI3Afg7v1Af5R1kkLV9vVIJfdx6O7uZu7cuZxyyikFyyZNmnT0uYd5X8SYU0zGbv/+/ZC9gfP/MLOPA9uBm9z9nfxyZrYYWAwwffr0SlczKhPMbAMwh+wdjRa5+5NhrTyuB/wwaFpmHNavXz/s9MOrr7565DZbmNl8srEuuHlu0igmY3f48GGADwL3uvs5wDvA8qHl3H2du7e6e+vUqYEvIFntNF1VIiX3Er3zzjs88sgjXH755UffW7t2LWvXrgVgw4YNzJkzB6AZuBtY6Am/p6FiUpr6+nqAfnffkntrAzA3uhrFQ7HpKnf/v5FWqopoWqZExx9/PG+8MbjTuWTJkqPPly1bxrJlyzCzjLufV+n6RUExKc2pp54K0G9ms9x9N/BpIBNtraIXZLoqpVNVgajnLhIPLwM/MrNngbOBb0Zcn8gFma5K6VRVIOq5i8TDu/ph12DDTFcVnIuQ4kLruZvZifoRhoiEJX+6KveWpqvGIMye+11kz2pfaWa1ZIdTIiLjcWS6qhbYB/zHiOtTNUJJ7mZ2AnAh8FUo348wkvydVBEpStNVJQprWuZ04BDZs9o7zOx7ZnZ8SOsWEZExCiu515D9Xu6IP8JI4fVCREQiEdacey/QO9pZ7aRdR0WkmmhaM11C6bm7+6vAQZ3VFhGJhzC/LfOf0FltEZFYCC25u/tOQGe1RURiQJcfEBFJICV3EZEE0rVlUq7a7i4jIsGo5y4ikkBK7iIiCaTkLiKSQJpzTyj9GlEk3ZTcRURKFOcvJGhaRkQkgRLbcy/3EbWxsZGJEycyYcIEampq6OnpGbTc3bnpppsA5uTui/lVd386tArEVJC4AA1mthf4LSmJi0ilJTa5V8Jjjz3GlClTii7r7u5mz549AM8Dy4B7gXMrV7vojBYX4APAqWTjkZq4jMbMJgA9QJ+7fyHq+kh1U3Ivk87OTq699lo2btyIuz+Vu8fsNHd/Jeq6RamzsxPgDc924RWXwW4CXgAmRV2RYuI8vyyFlNxLZGZ87nOfw8y48cYbWbx48aDlfX19NDQ05L/VC9QBiU5iQeLC4FswFo2LmS0GFgNMnz69rHUearhvGpU5kR0HXAJ8A7i5nBuqNma2A41mxkzJvURPPPEEdXV1vPbaa3z2s5/lYx/7GBdeeOGY1xNlEiuHsOKSwhu7NACLgIlRVyRmTgH+jZiOZuJM35YpUV1dHQAnn3wyl112GVu3bi1YfvDgwfy36oG+oetx93Xu3ururVOnTi1jjSsjSFyA2ry3isYlTR544AGAw+6+faRyabtNZW9vL8AJwPcirkpVUnIvwTvvvMNbb7119PnDDz/MnDlzBpVpb2/nBz/4AQBmdh7wZtLnlYPGBTjJslIRl9Fs3rwZ4EQzOwB0AJ8ys38YWi5pHYHRfO1rX4PstN37EVelKoWa3M1sgpntMLMHwlxv3PzqV7/i/PPP5+Mf/zjz58/nkksuYcGCBaxdu5a1a9cC0NbWxowZMwDmAN8F/jTCKldE0LgA7wF7SUlcRnPbbbcBPOvujcBC4Gfu/uVIKxWxBx54gJNPPhmyX5cdVtpGM2MR9px7rM/2h2XGjBk888wzBe8vWbLk6HMzY82aNdxzzz3Pu3sq7lAVNC7Ay2mJiZRm8+bNdHV1AZxJdjQzycz+YehBL4XnZgILreduZvVkz/ZrfkykRO7+uL4Vkh3N5Obcn0OjmZKE2XO/E/iv6Gy/xIAunCZpF0rP3cy+ALyms/0iEjaNZkoT1rTMJ4F2ne0XEYmHUJK7u3/d3et1tl9EJB70PXcRkQQK/fID7v448HjY6xURkeDUcxcRSaDYXjhMX2WTuNElb6WaqOcuIpJAse25i4hUo4juB1BAPXcRkQRSz73K6dyEFKN2IUruUiAuw0oRKZ2mZUREEkjJXUQkgZTcRUQSSMldRCSBlNxFRBIoVd+W0c/HRSQt1HMvwcGDB7noootobm6mpaWFu+66q6DM448/zgknnADQbGY7zWxFxStaQYqJSLwouZegpqaGb3/722QyGZ566inWrFlDJpMpKHfBBRcAZNz9bHdfWfGKVpBiUrqDBw8CfNTMMma2y8xuirpOUv2U3Eswbdo05s6dC8DEiROZPXs2fX19EdcqWopJ6WpqagB63b0ZOA9YambN0dYqejrojU+q5tzL4cCBA+zYsYNzzz23YNmTTz4J2SmIbuC/uPuuStcvCpWMSRJ+Zj9t2jSA3wK4+1tm9gJQBxQOfVIk/6BnZhOB7Wb2iLunOi5BhdJzN7MGM3ssbUfYt99+myuuuII777yTSZMmDVo2d+5cXnrpJcjuoH8P/GuxdZjZYjPrMbOeQ4cOlb3O5RZGTCB5cQnKzBqBc4AtRZalKiZDD3rAkYOeBBBWz/0w8Ofu/nRajrC/+93vuOKKK/jSl77E5ZdfXrA8P7G5+0Nmdo+ZTXH31/PLufs6YB1Aa2url7naZRVWTHLLExOXoMzsQ8A/AV9z998MXZ7GmBwx3EHPzBYDiwGmT58+4jqSMMobi1CSu7u/ArySe574YaW7c/311zN79mxuvvnmomVeffVVTjnlFADMbD7ZUdIblatl+Eb6KmlaYxIiI5vYf+Tu/xx1ZYKqxNeLRzropfmAN5rQ59xHG1YS8CgbZ5s3b+aHP/whZ555JmeffTYA3/zmN3n55ZcBWLJkCRs2bODee+8FaAbuBha6e2Ibn2JSulwITgP+zd3viLg6cVOVB704CDW5p2VYef755zNaTlq2bBnLli3DzDLufl6FqhYZxaR0mzdvBjgJ+JSZ7cy9/d/c/aHoahU9HfTGJ7TkbmbHoSOsyJidf/75ANvdvTXqusSJDnrjE0pyNzMD7gNe0BFWRMKQtINepS9/ElbP/ZPAV4DndIQVSTddwykewvq2zBNkT3yIiEgM6PIDIiIJpOQuIpJASu4iIgmk5C4ikkBK7iIiCRSLS/6m7YI+IiLlFovkHqVq+k6uDoLxU03tR9JF0zIiIgmk5C4ikkCpn5aR8dG0RPQ0XSfFKLmLhEwHPIkDTcuIiCSQkruISAJpWqaI4eYwNbQWkWqh5C5VQycORYJTcheRxKmWjkA5T76HNuduZgvMbLeZ7TWz5WGtN642btzIrFmzaGpqYtWqVQXL33vvPa6++mqAOWa2xcwaK1zFint333b6vnujYlJE4/IHiz7yTErT/jMGikuJwrqH6gRgDfBZoBfYZmZd7p4JY/1xcWRn9PcH+N/fvZGTr/4b9t/9FebNm0d7ezvNzc1Hy953331MnjwZ4Hng74DbgasjqHZF+PsD/PqRezn56r/hdxNPYsUd/5m7955A7ZTpQLY3kraYBDUwMAAwHWgmofvPWHrSR3quaYhLMWH15sPquc8H9rr7PnfvBzqAS0Nad+z0v/JLak6cxnEnnkptbS0LFy6ks7NzUJnOzk6uu+66Iy83AJ/O3Ug8kFF6ebGTHxObcBzHz76Qd/c8NajMeGOSVFu3bgV4Ly37T1CKy/iENedeBxzMe90LnBvSumPn8FtvUDNpKpBNwm8//zr9r+zmO29mE/CBVZfQ19dHQ0MDAO5+2MzeBE4CXo+o2mWVHxOACROn0P/K7kFl0haToPr6+gD6895K9P4TlOIyPhU9oWpmi4HFuZdvm9nukcqXYAqVSRSTgUlvP/vwS7ltvg986K2nH3wZwG4HoKWhoeGXwGkjrSiEmIz3M4cRsynAAMdiAvBhSowJlBSXsP7vK7qeXFwmAx8ZtWz5958jKrUfFWW3H93+qHGJoJ2Etb+MaR25dnLEqPsPAO4+7gfwCeCnea+/Dnw9jHWPsR49FdrO0c8L9BT7vMBPgU/kntfk/jMtbp85jJjlYjBqGyhnTML6v49iPXHZf8KOwXi3X464xGV/qUQcw5pz3wbMNLPTzawWWAh0hbTuODr6eQGj+OftAo5MMF8J/Mxz/7MJFaQNpC0mQaVt/wlKcRmHUKZlPDt/uoxsz2wC8N/dfVcY646jIZ93OvDX7r7LzFaSPSp3AfcBPzSzvcCvyTbMxBquDaQ5JkGlbf8JSnEZpyiHX2E/gMVp2GaY2w+j/lHHIMw6xG091RzLOG4/TfuL5TYmIiIJoqtCiogkUFUm99EudWBmN5tZxsyeNbNHzSzYV4fGsc28cleYmZtZ63i3GXTbZnahmT1tZofN7MoS1zFizAL8/RIze87MdprZE2bWPLRMGMKIRcD1BGpDcYlLKaLYj4JuO6/cmPenOOwvAddR3rYR5dxaifNVE4AXgRlALfAM0DykzEXAB3PP/wT4cbm3mSs3Efg58BTQWsHP2wicBfwAuDLsmAX8+0l5z9uBjRH9348YizDbUFziUsZYhrofjWXbuXJj3p/isL/EpW1UY8991EsduPtj7v7b3MungPpybzPnr8leL+X/jXN7Y9q2ux9w92fJ/piq1HWMFLMgf/+bvJfHA+U4mRNGLIKuJ0gbiktcShHFfhR42zml7E9x2F+CrqOsbaMak3uxSx3UjVD+eqC73Ns0s7lAg7uHfRGYsX7eMNYxNGaB/t7MlprZi8DfAn82xjoGEUYsSlnPcG0oLnEpRRT7UeBtj2N/isP+Engd5Wwb1ZjcAzOzLwOtwLfKvJ3fA+4A/ryc26mE8cTM3de4+xnAXwB/GXbdohBGG6r2uFRqP8rbXtXsT+ONTTnbRjUm9z6gIe91fe69QczsM8CtQLu7v1dOZVNoAAABIUlEQVTmbU4E5gCPm9kB4DygK6STqoE+bxjrGCFmY61DB/BHY6xjEGHEIvB6ArShuMSlFFHsR0G3PZ79KQ77Syn1CL9thDmBX4kH2V/V7gNO59iJipYhZc4hezJjZqW2OaT844R3QjXwtoHvU/wE0bhiFvDvZ+Y9/w+U4foZYcQizDYUl7iUK5Zh70el/D/mygfen+Kwv8SlbUTeyEpsHG3AL3PBvTX33kqyR1CATcCvgJ25R1e5t1lqYwzp884jO6f3DvAGsCvsmAX4+7uAXbm/fWyknTXqWITZhuISl2rZjyqxP8Vhf4lD29AvVEVEEqga59xFRGQUSu4iIgmk5C4ikkBK7iIiCaTkLiKSQEruIiIJpOQuIpJASu4iIgn0/wEgzBSlrTt1dwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average val loss: 1.5487115383148193\n",
      "Training with lr 1e-05...\n",
      "   11 / 15002: loss : 1.9680 -- perceptual loss : 0.2655 -- style loss : 1.7025\r"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "val_step = 0\n",
    "for (content_image, content_path), (style_image, style_path) in data_loader_train:\n",
    "    if iteration >= ITERATIONS: \n",
    "        break\n",
    "    if torch.cuda.is_available():\n",
    "        content_image = content_image.to('cuda')\n",
    "        style_image = style_image.to('cuda')\n",
    "    \n",
    "    content_encoder.train(), style_encoder.train(), decoder.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    stylized, content_encoding, style_encoding = forward(content_image, style_image, content_encoder, style_encoder, decoder)\n",
    "        \n",
    "    features_content = loss_net(content_image)\n",
    "    features_style = loss_net(style_image)\n",
    "    features_stylized = loss_net(stylized)\n",
    "    \n",
    "    perceptual_loss = loss.perceptual_loss(features_content, features_stylized, CONTENT_LOSS_WEIGHTS, loss=LOSS_TYPE)\n",
    "    style_loss = loss.style_loss(features_style, features_stylized, STYLE_LOSS_WEIGHTS, loss=LOSS_TYPE)\n",
    "    total_loss = perceptual_loss + style_loss\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    tb_writer.add_scalar('train loss', total_loss.item(), iteration)\n",
    "    tb_writer.add_scalar('train perceptual loss', perceptual_loss.item(), iteration)\n",
    "    tb_writer.add_scalar('train style loss', style_loss.item(), iteration)\n",
    "    print(f'\\r{iteration:5d} / {ITERATIONS}: loss : {total_loss.item():.4f} -- perceptual loss : {perceptual_loss.item():.4f} -- style loss : {style_loss.item():.4f}', end='\\r')\n",
    "    \n",
    "    if iteration % 100 == 0:\n",
    "        #tb_writer.add_images('train images', torch.from_numpy(np.concatenate([\n",
    "        #    data.vgg_normalization_undo(img.detach().cpu().numpy()) for img in [content_image, style_image, stylized]\n",
    "        #])), iteration)\n",
    "        # Validate\n",
    "        print('\\nValidation...')\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            val_iteration = 0\n",
    "            content_encoder.eval(), style_encoder.eval(), decoder.eval()\n",
    "            fig = plt.figure()\n",
    "            for (content_image, content_path), (style_image, style_path) in data_loader_val:\n",
    "                \n",
    "                if val_iteration >= VAL_ITERATIONS:\n",
    "                    break\n",
    "                    \n",
    "                if torch.cuda.is_available():\n",
    "                    content_image = content_image.to('cuda')\n",
    "                    style_image = style_image.to('cuda')\n",
    "                    \n",
    "                stylized, content_encoding, style_encoding = forward(content_image, style_image, content_encoder, style_encoder, decoder)\n",
    "                \n",
    "                torch.set_printoptions(profile=\"full\")\n",
    "                #print(content_encoding)\n",
    "                torch.set_printoptions(profile=\"default\")\n",
    "    \n",
    "                features_content = loss_net(content_image)\n",
    "                features_style = loss_net(style_image)\n",
    "                features_stylized = loss_net(stylized)\n",
    "\n",
    "                perceptual_loss = loss.perceptual_loss(features_content, features_stylized, CONTENT_LOSS_WEIGHTS, loss=LOSS_TYPE)\n",
    "                style_loss = loss.style_loss(features_style, features_stylized, STYLE_LOSS_WEIGHTS, loss=LOSS_TYPE)\n",
    "                total_loss = perceptual_loss + style_loss\n",
    "                total_val_loss += total_loss\n",
    "                                \n",
    "                #plt.hist(content_encoding.detach().cpu().numpy().reshape(-1), bins=np.arange(-10, 10.05, 0.05), range=[-10.0, 10.0], density=True)\n",
    "                #plt.hist(content_encoding.detach().cpu().numpy().reshape(-1), density=True)\n",
    "                #plt.show()\n",
    "                fig.add_subplot(2, VAL_ITERATIONS, val_iteration + 1)\n",
    "                plt.hist(style_encoding.detach().cpu().numpy().reshape(-1), density=True)\n",
    "                fig.add_subplot(2, VAL_ITERATIONS, VAL_ITERATIONS + val_iteration + 1)\n",
    "                plt.hist(style_encoding.detach().cpu().numpy().std(0), density=True)\n",
    "                \n",
    "                #content_mean = np.mean(content_encoding.detach().cpu().numpy(), axis=(1))\n",
    "                #content_std = np.std(content_encoding.detach().cpu().numpy(), axis=(1))\n",
    "                \n",
    "                #print(\"content mean: {}\", content_mean)\n",
    "                #print(\"content std: {}\", content_std)\n",
    "\n",
    "                #tb_writer.add_histogram('embedding distribution', content_encoding.detach().cpu().numpy(), val_step, bins=1000)\n",
    "                tb_writer.add_scalar('validation loss', total_loss.item(), val_step)\n",
    "                tb_writer.add_scalar('validation perceptual loss', perceptual_loss.item(), val_step)\n",
    "                tb_writer.add_scalar('validation style loss', style_loss.item(), val_step)\n",
    "                tb_writer.add_images('validation images', torch.from_numpy(np.concatenate([\n",
    "                    img.detach().cpu().numpy() for img in [content_image, style_image, stylized]\n",
    "                ])), val_step)\n",
    "                val_iteration += 1\n",
    "                val_step += 1\n",
    "                print(f'\\r{val_iteration:5d} / {VAL_ITERATIONS}: loss : {total_loss.item():.4f} -- perceptual loss : {perceptual_loss.item():.4f} -- style loss : {style_loss.item():.4f}', end='\\r')\n",
    "            \n",
    "            plt.show()\n",
    "            total_val_loss /= VAL_ITERATIONS\n",
    "            print(f'\\nAverage val loss: {total_val_loss}')\n",
    "            lr_scheduler.step(total_val_loss)\n",
    "            print(f'Training with lr {optimizer.param_groups[0][\"lr\"]}...')\n",
    "            \n",
    "            \n",
    "    iteration += 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(trainable_parameters, lr=1e-6)\n",
    "\n",
    "\n",
    "iteration = 0\n",
    "running_perceptual_loss, running_style_loss, running_count = 0.0, 0.0, 0\n",
    "\n",
    "content_loss_history_train = []\n",
    "style_loss_history_train = []\n",
    "content_loss_history_val = []\n",
    "style_loss_history_val = []\n",
    "\n",
    "for (content_image, content_path), (style_image, style_path) in data_loader_train:\n",
    "    if iteration >= ITERATIONS: break\n",
    "    iteration += 1\n",
    "    \n",
    "    content_encoder.train()\n",
    "    style_encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    if torch.cuda.is_available():\n",
    "        content_image = content_image.to('cuda')\n",
    "        style_image = style_image.to('cuda')\n",
    "\n",
    "    #content_representation = content_encoder(content_image)\n",
    "    #style_representation = style_encoder(style_image)\n",
    "\n",
    "    #transformed_content = function.adain(content_representation, style_representation)\n",
    "    #transformed = decoder(transformed_content)\n",
    "    #transformed = decoder(content_representation, style_representation)\n",
    "    \n",
    "    transformed, style_representation = forward(content_image, style_image, content_encoder, style_encoder, decoder)\n",
    "\n",
    "    features_content = loss_net(content_image)\n",
    "    features_style = loss_net(style_image)\n",
    "    features_transformed = loss_net(transformed)\n",
    "\n",
    "    perceptual_loss = loss.perceptual_loss(features_content, features_transformed, CONTENT_LOSS_WEIGHTS)\n",
    "    style_loss = loss.style_loss(features_style, features_transformed, STYLE_LOSS_WEIGHTS)\n",
    "    \n",
    "    lambda_content = 1.0\n",
    "    lambda_style = 1.0\n",
    "\n",
    "    total_loss = lambda_content * perceptual_loss + lambda_style * style_loss\n",
    "\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    running_perceptual_loss += perceptual_loss.item()\n",
    "    running_style_loss += style_loss.item()\n",
    "\n",
    "    running_count += 1\n",
    "\n",
    "    print(f'\\r{iteration:06d} : avg perceptual_loss : {running_perceptual_loss / running_count:.4f}\\tavg style loss : {running_style_loss / running_count:.4f}', end='\\r')\n",
    "    content_loss_history_train.append(perceptual_loss.item())\n",
    "    style_loss_history_train.append(style_loss.item())\n",
    "    \n",
    "    if iteration % 5000 == 1:\n",
    "        torch.save(content_encoder.state_dict(), f'output/content_encoder_{iteration}')\n",
    "        torch.save(style_encoder.state_dict(), f'output/style_encoder_{iteration}')\n",
    "        torch.save(decoder.state_dict(), f'output/decoder_{iteration}')\n",
    "\n",
    "    if iteration % 500 == 1:\n",
    "\n",
    "        running_perceptual_loss, running_style_loss, running_count = 0.0, 0.0, 0 # After each validation, reset running training losses\n",
    "        print(f'\\nValidating...')\n",
    "\n",
    "        content_encoder.eval()\n",
    "        style_encoder.eval()\n",
    "        decoder.eval()\n",
    "        perceptual_loss = 0.0\n",
    "        style_loss = 0.0\n",
    "        val_iteration = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            torch.save(content_image.cpu(), f'output/{iteration}_0_content.pt')\n",
    "            torch.save(style_image.cpu(), f'output/{iteration}_0_style.pt')\n",
    "            #torch.save(decoder(style_representation).cpu(), f'output/{iteration}_0_style_reconstructed.pt')\n",
    "            #torch.save(decoder(content_representation).cpu(), f'output/{iteration}_0_reconstructed.pt')\n",
    "            torch.save(transformed.cpu(), f'output/{iteration}_0_transformed.pt')\n",
    "\n",
    "\n",
    "            for (content_image, content_path), (style_image, style_path) in data_loader_val:\n",
    "                val_iteration += 1\n",
    "                if val_iteration > VAL_ITERATIONS: break\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    content_image = content_image.to('cuda')\n",
    "                    style_image = style_image.to('cuda')\n",
    "\n",
    "                #content_representation = content_encoder(content_image)\n",
    "                #style_representation = style_encoder(style_image)\n",
    "\n",
    "                #transformed_content = function.adain(content_representation, style_representation)\n",
    "                #reconstruction = decoder(transformed_content)\n",
    "                #reconstruction = decoder(content_representation, style_representation)\n",
    "                \n",
    "                reconstruction, style_representation = forward(content_image, style_image, content_encoder, style_encoder, decoder)\n",
    "\n",
    "                features_content = loss_net(content_image)\n",
    "                features_style = loss_net(style_image)\n",
    "                features_reconstruction = loss_net(reconstruction)\n",
    "                \n",
    "                print(\"------------------------------------------------------------------\")\n",
    "                print(\"Features content:\")\n",
    "                print_content_feature_contrib(features_content, features_reconstruction)\n",
    "                print(\"Features style:\")\n",
    "                print_style_feature_contrib(features_style, features_reconstruction)\n",
    "                print(\"------------------------------------------------------------------\")\n",
    "                \n",
    "                perceptual_loss += loss.perceptual_loss(features_content, features_reconstruction, CONTENT_LOSS_WEIGHTS)\n",
    "                style_loss += loss.style_loss(features_style, features_reconstruction, STYLE_LOSS_WEIGHTS)\n",
    "                    \n",
    "                torch.save(content_image.cpu(), f'output/{iteration}_{val_iteration}_content.pt')\n",
    "                torch.save(style_image.cpu(), f'output/{iteration}_{val_iteration}_style.pt')\n",
    "                torch.save(reconstruction.cpu(), f'output/{iteration}_{val_iteration}_reconstruction.pt')\n",
    "                #torch.save(decoder(style_representation).cpu(), f'output/{iteration}_{val_iteration}_style_reconstruction.pt')\n",
    "\n",
    "                print(f'\\rValidation {val_iteration:02d} : Perceptual loss {perceptual_loss / val_iteration:.4f}\\tStyle loss {style_loss / val_iteration:.4f}', end='\\r')\n",
    "            print('\\nValidation done.')\n",
    "            val_iteration -= 1\n",
    "            content_loss_history_val.append((iteration, perceptual_loss / val_iteration))\n",
    "            style_loss_history_val.append((iteration, style_loss / val_iteration))\n",
    "\n",
    "            torch.save(content_loss_history_train, 'output/content_loss_history_train.pt')\n",
    "            torch.save(style_loss_history_train, 'output/style_loss_history_train.pt')\n",
    "            torch.save(content_loss_history_val, 'output/content_loss_history_val.pt')\n",
    "            torch.save(style_loss_history_val, 'output/style_loss_history_val.pt')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTENT_LOSS_WEIGHTS = {\n",
    "    'relu_4_2' : 5e-2,\n",
    "}\n",
    "\n",
    "STYLE_LOSS_WEIGHTS = {\n",
    "    'relu_1_1' : 1e3,\n",
    "    'relu_2_1' : 1e3,\n",
    "    'relu_3_1' : 1e3,\n",
    "    'relu_4_1' : 1e3,\n",
    "    'relu_5_1' : 1e3,\n",
    "}\n",
    "RESOLUTION = 256\n",
    "ITERATION = 5001\n",
    "EVAL_ITERATIONS = 1\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "data_style = data.load_debug_dataset('../dataset/debug/style', resolution=RESOLUTION)\n",
    "data_loader_style_eval = DataLoader(data_style, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "\n",
    "data_content = data.load_debug_dataset('../dataset/debug/content', resolution=RESOLUTION)\n",
    "data_loader_content_eval = DataLoader(data_content, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "\n",
    "data_loader_eval = data.DatasetPairIterator(data_loader_content_eval, data_loader_style_eval)\n",
    "\n",
    "content_encoder = model.Encoder((3, RESOLUTION, RESOLUTION), pretrained=True)\n",
    "#style_encoder = model.Encoder((3, RESOLUTION, RESOLUTION), pretrained=True, flattened_output_dim=STYLE_DIM)\n",
    "style_encoder = model.Encoder((3, RESOLUTION, RESOLUTION), pretrained=True)\n",
    "#decoder = model.Decoder(STYLE_DIM)\n",
    "decoder = model.Decoder()\n",
    "\n",
    "content_encoder.load_state_dict(torch.load(f'output_train_places365_starry_night/content_encoder_{ITERATION}'))\n",
    "style_encoder.load_state_dict(torch.load(f'output_train_places365_starry_night/style_encoder_{ITERATION}'))\n",
    "decoder.load_state_dict(torch.load(f'output_train_places365_starry_night/decoder_{ITERATION}'))\n",
    "\n",
    "loss_net = loss.LossNet()\n",
    "loss_net.eval()\n",
    "\n",
    "# Networks to CUDA device\n",
    "if torch.cuda.is_available(): \n",
    "    content_encoder = content_encoder.cuda()\n",
    "    style_encoder = style_encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "    loss_net = loss_net.cuda()\n",
    "\n",
    "content_encoder.eval()\n",
    "style_encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    eval_iteration = 0\n",
    "\n",
    "    for (content_image, content_path), (style_image, style_path) in data_loader_eval:\n",
    "        if eval_iteration >= EVAL_ITERATIONS:\n",
    "            break\n",
    "\n",
    "        perceptual_loss = 0.0\n",
    "        style_loss = 0.0\n",
    "        eval_iteration += 1\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            content_image = content_image.to('cuda')\n",
    "            style_image = style_image.to('cuda')\n",
    "\n",
    "        #content_representation = content_encoder(content_image)\n",
    "        #style_representation = style_encoder(style_image)\n",
    "\n",
    "        #transformed_content = function.adain(content_representation, style_representation)\n",
    "        #reconstruction = decoder(transformed_content)\n",
    "        #reconstruction = decoder(content_representation, style_representation)\n",
    "\n",
    "        reconstruction, style_representation = forward(content_image, style_image, content_encoder, style_encoder, decoder)\n",
    "\n",
    "        features_content = loss_net(content_image)\n",
    "        features_style = loss_net(style_image)\n",
    "        features_reconstruction = loss_net(reconstruction)\n",
    "\n",
    "        perceptual_loss += loss.perceptual_loss(features_content, features_reconstruction, CONTENT_LOSS_WEIGHTS)\n",
    "        style_loss += loss.style_loss(features_style, features_reconstruction, STYLE_LOSS_WEIGHTS)\n",
    "\n",
    "        torch.save(content_image.cpu(), f'output/eval_{eval_iteration}_content.pt')\n",
    "        torch.save(style_image.cpu(), f'output/eval_{eval_iteration}_style.pt')\n",
    "        torch.save(reconstruction.cpu(), f'output/eval_{eval_iteration}_reconstruction.pt')\n",
    "\n",
    "        print(f'Evaluation {eval_iteration:02d} : Perceptual loss {perceptual_loss:.4f}\\tStyle loss {style_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(content_loss_history_train[10:], \"r\")\n",
    "plt.plot(style_loss_history_train[10:], \"b\")\n",
    "plt.plot(*zip(*content_loss_history_val), \"r--\")\n",
    "plt.plot(*zip(*style_loss_history_val), \"b--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch = torch.load(\"./output/91_9_reconstruction.pt\")\n",
    "image_batch = data.vgg_normalization_undo(image_batch.numpy())\n",
    "#plt.imshow(transforms.ToPILImage()(image))\n",
    "image_batch = np.transpose(image_batch, (0, 2, 3, 1))\n",
    "plt.imshow(image_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(epoch, iteration):\n",
    "    content_batch = torch.load(\"./output/{}_{}_content.pt\".format(epoch, iteration))\n",
    "    style_batch = torch.load(\"./output/{}_{}_style.pt\".format(epoch, iteration))\n",
    "    reconstruction_batch = torch.load(\"./output/{}_{}_reconstruction.pt\".format(epoch, iteration))\n",
    "    \n",
    "    content_batch = data.vgg_normalization_undo(content_batch.numpy())\n",
    "    style_batch = data.vgg_normalization_undo(style_batch.numpy())\n",
    "    reconstruction_batch = data.vgg_normalization_undo(reconstruction_batch.numpy())\n",
    "    \n",
    "    content_batch = np.transpose(content_batch, (0, 2, 3, 1))\n",
    "    style_batch = np.transpose(style_batch, (0, 2, 3, 1))\n",
    "    reconstruction_batch = np.transpose(reconstruction_batch, (0, 2, 3, 1))\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    fig.add_subplot(1, 3, 1)\n",
    "    plt.imshow(content_batch[0])\n",
    "    \n",
    "    fig.add_subplot(1, 3, 2)\n",
    "    plt.imshow(style_batch[0])\n",
    "    \n",
    "    fig.add_subplot(1, 3, 3)\n",
    "    plt.imshow(reconstruction_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(STYLE_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0, 1.05, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.resize_images_offline(\"../dataset/style\", \"../dataset/style_resized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.resize_images_offline(\"../dataset/debug/style\", \"../dataset/debug/style_resized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
