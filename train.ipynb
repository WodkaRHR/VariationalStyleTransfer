{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import data, model.unet, model.autoencoder, loss, function\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torch.utils.tensorboard as tb\n",
    "import torchvision\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_writer = tb.SummaryWriter('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_PORTION = 0.1\n",
    "ITERATIONS = 15002\n",
    "VAL_ITERATIONS = 5\n",
    "RESOLUTION = 64\n",
    "CHANNELS = 3\n",
    "STYLE_DIM = 512\n",
    "#CONTENT_DIM = 512\n",
    "BATCH_SIZE = 8\n",
    "LOSS_TYPE = 'l1'\n",
    "#CONTENT_LOSS_WEIGHTS = {'input' : 0.0, 'relu3' : 0.0, 'relu4' : 1e2, 'relu5' : 0.0}\n",
    "#STYLE_LOSS_WEIGHTS = {'input' : 0.0, 'relu1' : 1e2, 'relu2' : 1e2, 'relu3' : 1e2, 'relu4' : 1e2, 'relu5' : 1e2}\n",
    "\n",
    "CONTENT_LOSS_WEIGHTS = {\n",
    "    #'relu_1_1' : 1e-2,\n",
    "    #'relu_4_2' : 5e-3,\n",
    "    'relu_4_2' : 5e-1,\n",
    "}\n",
    "\n",
    "STYLE_LOSS_WEIGHTS = {\n",
    "    'relu_1_1' : 1e2,\n",
    "    'relu_2_1' : 1e2,\n",
    "    'relu_3_1' : 1e2,\n",
    "    'relu_4_1' : 1e2,\n",
    "    'relu_5_1' : 1e2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_style = data.load_debug_dataset('../dataset/debug/style', resolution=RESOLUTION)\n",
    "data_content = data.load_debug_dataset('../dataset/debug/content', resolution=RESOLUTION)\n",
    "data_content_train, data_content_val = torch.utils.data.random_split(data_content, [len(data_content) - int(VAL_PORTION * len(data_content)), int(VAL_PORTION * len(data_content))])\n",
    "data_style_train, data_style_val = torch.utils.data.random_split(data_style, [len(data_style) - int(VAL_PORTION * len(data_style)), int(VAL_PORTION * len(data_style))])\n",
    "data_loader_content_train = DataLoader(data_content_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "data_loader_content_val = DataLoader(data_content_val, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "data_loader_style_train = DataLoader(data_style_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "data_loader_style_val = DataLoader(data_style_val, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "data_loader_train = data.DatasetPairIterator(data_loader_content_train, data_loader_style_train)\n",
    "data_loader_val = data.DatasetPairIterator(data_loader_content_val, data_loader_style_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTENT_DOWN_CONVOLUTIONS = 3\n",
    "STYLE_DOWN_CONVOLUTIONS = 4\n",
    "\n",
    "normalization = ['adain', 'adain', None, None, None]\n",
    "unet = model.unet.UNetAutoencoder(3, STYLE_DIM, residual_downsampling=True, residual_adain=True, residual_upsampling=True, \n",
    "        down_normalization='in', up_normalization='adain', num_adain_convolutions=CONTENT_DOWN_CONVOLUTIONS, \n",
    "        num_downup_convolutions=3, output_activation='sigmoid')\n",
    "style_encoder = model.autoencoder.Encoder(STYLE_DIM, num_down_convolutions=CONTENT_DOWN_CONVOLUTIONS)\n",
    "\n",
    "\n",
    "loss_net = loss.LossNet()\n",
    "_ = loss_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): \n",
    "    unet = unet.cuda()\n",
    "    style_encoder = style_encoder.cuda()\n",
    "    loss_net = loss_net.cuda()\n",
    "\n",
    "trainable_parameters = []\n",
    "for parameter in unet.parameters():\n",
    "    trainable_parameters.append(parameter)\n",
    "for parameter in style_encoder.parameters():\n",
    "    trainable_parameters.append(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(trainable_parameters, lr=5e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 / 15002: loss : 3.5048 -- perceptual loss : 1.0030 -- style loss : 2.5018\n",
      "Validation...\n",
      "    4 / 5: loss : 3.2798 -- perceptual loss : 0.9973 -- style loss : 2.2824\r"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "val_step = 0\n",
    "for (content_image, content_path), (style_image, style_path) in data_loader_train:\n",
    "    if iteration >= ITERATIONS: \n",
    "        break\n",
    "    if torch.cuda.is_available():\n",
    "        content_image = content_image.to('cuda')\n",
    "        style_image = style_image.to('cuda')\n",
    "    \n",
    "    unet.train(), style_encoder.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    style_encoding = style_encoder(style_image)\n",
    "    stylized = unet(content_image, style_encoding)\n",
    "    \n",
    "    features_content = loss_net(content_image)\n",
    "    features_style = loss_net(style_image)\n",
    "    features_stylized = loss_net(stylized)\n",
    "    \n",
    "    perceptual_loss = loss.perceptual_loss(features_content, features_stylized, CONTENT_LOSS_WEIGHTS, loss=LOSS_TYPE)\n",
    "    style_loss = loss.style_loss(features_style, features_stylized, STYLE_LOSS_WEIGHTS, loss=LOSS_TYPE)\n",
    "    total_loss = perceptual_loss + style_loss\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    tb_writer.add_scalar('train loss', total_loss.item(), iteration)\n",
    "    tb_writer.add_scalar('train perceptual loss', perceptual_loss.item(), iteration)\n",
    "    tb_writer.add_scalar('train style loss', style_loss.item(), iteration)\n",
    "    print(f'\\r{iteration:5d} / {ITERATIONS}: loss : {total_loss.item():.4f} -- perceptual loss : {perceptual_loss.item():.4f} -- style loss : {style_loss.item():.4f}', end='\\r')\n",
    "    \n",
    "    if iteration % 100 == 0:\n",
    "        #tb_writer.add_images('train images', torch.from_numpy(np.concatenate([\n",
    "        #    data.vgg_normalization_undo(img.detach().cpu().numpy()) for img in [content_image, style_image, stylized]\n",
    "        #])), iteration)\n",
    "        # Validate\n",
    "        print('\\nValidation...')\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            val_iteration = 0\n",
    "            unet.eval(), style_encoder.eval()\n",
    "            fig = plt.figure()\n",
    "            for (content_image, content_path), (style_image, style_path) in data_loader_val:\n",
    "                \n",
    "                if val_iteration >= VAL_ITERATIONS:\n",
    "                    break\n",
    "                    \n",
    "                if torch.cuda.is_available():\n",
    "                    content_image = content_image.to('cuda')\n",
    "                    style_image = style_image.to('cuda')\n",
    "                \n",
    "                style_encoding = style_encoder(style_image)\n",
    "                stylized = unet(content_image, style_encoding)\n",
    "                \n",
    "                torch.set_printoptions(profile=\"full\")\n",
    "                #print(content_encoding)\n",
    "                torch.set_printoptions(profile=\"default\")\n",
    "    \n",
    "                features_content = loss_net(content_image)\n",
    "                features_style = loss_net(style_image)\n",
    "                features_stylized = loss_net(stylized)\n",
    "\n",
    "                perceptual_loss = loss.perceptual_loss(features_content, features_stylized, CONTENT_LOSS_WEIGHTS, loss=LOSS_TYPE)\n",
    "                style_loss = loss.style_loss(features_style, features_stylized, STYLE_LOSS_WEIGHTS, loss=LOSS_TYPE)\n",
    "                total_loss = perceptual_loss + style_loss\n",
    "                total_val_loss += total_loss\n",
    "                fig.add_subplot(2, VAL_ITERATIONS, val_iteration + 1)\n",
    "                plt.hist(style_encoding.detach().cpu().numpy().reshape(-1), density=True)\n",
    "                fig.add_subplot(2, VAL_ITERATIONS, VAL_ITERATIONS + val_iteration + 1)\n",
    "                plt.hist(style_encoding.detach().cpu().numpy().std(0), density=True)\n",
    "                \n",
    "                tb_writer.add_scalar('validation loss', total_loss.item(), val_step)\n",
    "                tb_writer.add_scalar('validation perceptual loss', perceptual_loss.item(), val_step)\n",
    "                tb_writer.add_scalar('validation style loss', style_loss.item(), val_step)\n",
    "                tb_writer.add_images('validation images', torch.from_numpy(np.concatenate([\n",
    "                    img.detach().cpu().numpy() for img in [content_image, style_image, stylized]\n",
    "                ])), val_step)\n",
    "                val_iteration += 1\n",
    "                val_step += 1\n",
    "                print(f'\\r{val_iteration:5d} / {VAL_ITERATIONS}: loss : {total_loss.item():.4f} -- perceptual loss : {perceptual_loss.item():.4f} -- style loss : {style_loss.item():.4f}', end='\\r')\n",
    "            \n",
    "            plt.show()\n",
    "            total_val_loss /= VAL_ITERATIONS\n",
    "            print(f'\\nAverage val loss: {total_val_loss}')\n",
    "            lr_scheduler.step(total_val_loss)\n",
    "            print(f'Training with lr {optimizer.param_groups[0][\"lr\"]}...')\n",
    "            \n",
    "            \n",
    "    iteration += 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(trainable_parameters, lr=1e-6)\n",
    "\n",
    "\n",
    "iteration = 0\n",
    "running_perceptual_loss, running_style_loss, running_count = 0.0, 0.0, 0\n",
    "\n",
    "content_loss_history_train = []\n",
    "style_loss_history_train = []\n",
    "content_loss_history_val = []\n",
    "style_loss_history_val = []\n",
    "\n",
    "for (content_image, content_path), (style_image, style_path) in data_loader_train:\n",
    "    if iteration >= ITERATIONS: break\n",
    "    iteration += 1\n",
    "    \n",
    "    content_encoder.train()\n",
    "    style_encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    if torch.cuda.is_available():\n",
    "        content_image = content_image.to('cuda')\n",
    "        style_image = style_image.to('cuda')\n",
    "\n",
    "    #content_representation = content_encoder(content_image)\n",
    "    #style_representation = style_encoder(style_image)\n",
    "\n",
    "    #transformed_content = function.adain(content_representation, style_representation)\n",
    "    #transformed = decoder(transformed_content)\n",
    "    #transformed = decoder(content_representation, style_representation)\n",
    "    \n",
    "    transformed, style_representation = forward(content_image, style_image, content_encoder, style_encoder, decoder)\n",
    "\n",
    "    features_content = loss_net(content_image)\n",
    "    features_style = loss_net(style_image)\n",
    "    features_transformed = loss_net(transformed)\n",
    "\n",
    "    perceptual_loss = loss.perceptual_loss(features_content, features_transformed, CONTENT_LOSS_WEIGHTS)\n",
    "    style_loss = loss.style_loss(features_style, features_transformed, STYLE_LOSS_WEIGHTS)\n",
    "    \n",
    "    lambda_content = 1.0\n",
    "    lambda_style = 1.0\n",
    "\n",
    "    total_loss = lambda_content * perceptual_loss + lambda_style * style_loss\n",
    "\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    running_perceptual_loss += perceptual_loss.item()\n",
    "    running_style_loss += style_loss.item()\n",
    "\n",
    "    running_count += 1\n",
    "\n",
    "    print(f'\\r{iteration:06d} : avg perceptual_loss : {running_perceptual_loss / running_count:.4f}\\tavg style loss : {running_style_loss / running_count:.4f}', end='\\r')\n",
    "    content_loss_history_train.append(perceptual_loss.item())\n",
    "    style_loss_history_train.append(style_loss.item())\n",
    "    \n",
    "    if iteration % 5000 == 1:\n",
    "        torch.save(content_encoder.state_dict(), f'output/content_encoder_{iteration}')\n",
    "        torch.save(style_encoder.state_dict(), f'output/style_encoder_{iteration}')\n",
    "        torch.save(decoder.state_dict(), f'output/decoder_{iteration}')\n",
    "\n",
    "    if iteration % 500 == 1:\n",
    "\n",
    "        running_perceptual_loss, running_style_loss, running_count = 0.0, 0.0, 0 # After each validation, reset running training losses\n",
    "        print(f'\\nValidating...')\n",
    "\n",
    "        content_encoder.eval()\n",
    "        style_encoder.eval()\n",
    "        decoder.eval()\n",
    "        perceptual_loss = 0.0\n",
    "        style_loss = 0.0\n",
    "        val_iteration = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            torch.save(content_image.cpu(), f'output/{iteration}_0_content.pt')\n",
    "            torch.save(style_image.cpu(), f'output/{iteration}_0_style.pt')\n",
    "            #torch.save(decoder(style_representation).cpu(), f'output/{iteration}_0_style_reconstructed.pt')\n",
    "            #torch.save(decoder(content_representation).cpu(), f'output/{iteration}_0_reconstructed.pt')\n",
    "            torch.save(transformed.cpu(), f'output/{iteration}_0_transformed.pt')\n",
    "\n",
    "\n",
    "            for (content_image, content_path), (style_image, style_path) in data_loader_val:\n",
    "                val_iteration += 1\n",
    "                if val_iteration > VAL_ITERATIONS: break\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    content_image = content_image.to('cuda')\n",
    "                    style_image = style_image.to('cuda')\n",
    "\n",
    "                #content_representation = content_encoder(content_image)\n",
    "                #style_representation = style_encoder(style_image)\n",
    "\n",
    "                #transformed_content = function.adain(content_representation, style_representation)\n",
    "                #reconstruction = decoder(transformed_content)\n",
    "                #reconstruction = decoder(content_representation, style_representation)\n",
    "                \n",
    "                reconstruction, style_representation = forward(content_image, style_image, content_encoder, style_encoder, decoder)\n",
    "\n",
    "                features_content = loss_net(content_image)\n",
    "                features_style = loss_net(style_image)\n",
    "                features_reconstruction = loss_net(reconstruction)\n",
    "                \n",
    "                print(\"------------------------------------------------------------------\")\n",
    "                print(\"Features content:\")\n",
    "                print_content_feature_contrib(features_content, features_reconstruction)\n",
    "                print(\"Features style:\")\n",
    "                print_style_feature_contrib(features_style, features_reconstruction)\n",
    "                print(\"------------------------------------------------------------------\")\n",
    "                \n",
    "                perceptual_loss += loss.perceptual_loss(features_content, features_reconstruction, CONTENT_LOSS_WEIGHTS)\n",
    "                style_loss += loss.style_loss(features_style, features_reconstruction, STYLE_LOSS_WEIGHTS)\n",
    "                    \n",
    "                torch.save(content_image.cpu(), f'output/{iteration}_{val_iteration}_content.pt')\n",
    "                torch.save(style_image.cpu(), f'output/{iteration}_{val_iteration}_style.pt')\n",
    "                torch.save(reconstruction.cpu(), f'output/{iteration}_{val_iteration}_reconstruction.pt')\n",
    "                #torch.save(decoder(style_representation).cpu(), f'output/{iteration}_{val_iteration}_style_reconstruction.pt')\n",
    "\n",
    "                print(f'\\rValidation {val_iteration:02d} : Perceptual loss {perceptual_loss / val_iteration:.4f}\\tStyle loss {style_loss / val_iteration:.4f}', end='\\r')\n",
    "            print('\\nValidation done.')\n",
    "            val_iteration -= 1\n",
    "            content_loss_history_val.append((iteration, perceptual_loss / val_iteration))\n",
    "            style_loss_history_val.append((iteration, style_loss / val_iteration))\n",
    "\n",
    "            torch.save(content_loss_history_train, 'output/content_loss_history_train.pt')\n",
    "            torch.save(style_loss_history_train, 'output/style_loss_history_train.pt')\n",
    "            torch.save(content_loss_history_val, 'output/content_loss_history_val.pt')\n",
    "            torch.save(style_loss_history_val, 'output/style_loss_history_val.pt')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(content, style, content_encoder, style_encoder, decoder):\n",
    "    \"\"\" Forwards a batch through the pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    content : torch.Tensor, shape [B, C, H, W]\n",
    "        The content image.\n",
    "    style : torch.Tensor, shape [B, C, H', W']\n",
    "        The style image, usually H' = H and W' = W.\n",
    "    content_encoder : torch.nn.modules.Module\n",
    "        Encoder for content images.\n",
    "    style_encoder : torch.nn.modules.Module\n",
    "        Encoder for style images.\n",
    "    decoder : torch.nn.modules.Module\n",
    "        Decoder that uses AdaIn to decode the content and apply the style.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    reco : torch.Tensor, shape [B, C, H, W]\n",
    "        A reconstruction of the content after application of the style.\n",
    "    content_representation : torch.Tensor, shape [B, D]\n",
    "        Content representation.\n",
    "    style_representation : torch.Tensor, shape [B, D']\n",
    "        The latent style representation.\n",
    "    \"\"\"\n",
    "    content_representation = content_encoder(content)\n",
    "    style_representation = style_encoder(style)\n",
    "    #transformed_content = function.adain(content_representation, style_representation)\n",
    "    #print(content_representation.shape)\n",
    "    #print(style_representation.shape)\n",
    "    reco = decoder(content_representation, style_representation)\n",
    "    #reco = decoder(content_representation)\n",
    "    return reco, content_representation, style_representation\n",
    "\n",
    "def print_content_feature_contrib(feature_activations_x, feature_activations_y):\n",
    "    for key in feature_activations_x:\n",
    "        feature_loss = torch.nn.functional.mse_loss(feature_activations_x[key], feature_activations_y[key])\n",
    "        weight = 0.0\n",
    "        if key in CONTENT_LOSS_WEIGHTS:\n",
    "            weight = CONTENT_LOSS_WEIGHTS[key]\n",
    "        weighted_loss = weight * feature_loss\n",
    "        print(\"Content layer {} loss: {} weight: {} weighted loss: {}\".format(key, feature_loss, weight, weighted_loss))\n",
    "\n",
    "def print_style_feature_contrib(features_style, features_transformed):\n",
    "    for key, weight in STYLE_LOSS_WEIGHTS.items():\n",
    "        Gx = function.gram_matrix(features_style[key])\n",
    "        Gy = function.gram_matrix(features_transformed[key])\n",
    "        value = torch.nn.functional.mse_loss(Gx, Gy)\n",
    "        weighted_value = weight * value\n",
    "        print(f'Style loss {key} with weight {weight}: {value} weighted loss: {weighted_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTENT_LOSS_WEIGHTS = {\n",
    "    'relu_4_2' : 5e-2,\n",
    "}\n",
    "\n",
    "STYLE_LOSS_WEIGHTS = {\n",
    "    'relu_1_1' : 1e3,\n",
    "    'relu_2_1' : 1e3,\n",
    "    'relu_3_1' : 1e3,\n",
    "    'relu_4_1' : 1e3,\n",
    "    'relu_5_1' : 1e3,\n",
    "}\n",
    "RESOLUTION = 256\n",
    "ITERATION = 5001\n",
    "EVAL_ITERATIONS = 1\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "data_style = data.load_debug_dataset('../dataset/debug/style', resolution=RESOLUTION)\n",
    "data_loader_style_eval = DataLoader(data_style, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "\n",
    "data_content = data.load_debug_dataset('../dataset/debug/content', resolution=RESOLUTION)\n",
    "data_loader_content_eval = DataLoader(data_content, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "\n",
    "data_loader_eval = data.DatasetPairIterator(data_loader_content_eval, data_loader_style_eval)\n",
    "\n",
    "content_encoder = model.Encoder((3, RESOLUTION, RESOLUTION), pretrained=True)\n",
    "#style_encoder = model.Encoder((3, RESOLUTION, RESOLUTION), pretrained=True, flattened_output_dim=STYLE_DIM)\n",
    "style_encoder = model.Encoder((3, RESOLUTION, RESOLUTION), pretrained=True)\n",
    "#decoder = model.Decoder(STYLE_DIM)\n",
    "decoder = model.Decoder()\n",
    "\n",
    "content_encoder.load_state_dict(torch.load(f'output_train_places365_starry_night/content_encoder_{ITERATION}'))\n",
    "style_encoder.load_state_dict(torch.load(f'output_train_places365_starry_night/style_encoder_{ITERATION}'))\n",
    "decoder.load_state_dict(torch.load(f'output_train_places365_starry_night/decoder_{ITERATION}'))\n",
    "\n",
    "loss_net = loss.LossNet()\n",
    "loss_net.eval()\n",
    "\n",
    "# Networks to CUDA device\n",
    "if torch.cuda.is_available(): \n",
    "    content_encoder = content_encoder.cuda()\n",
    "    style_encoder = style_encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "    loss_net = loss_net.cuda()\n",
    "\n",
    "content_encoder.eval()\n",
    "style_encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    eval_iteration = 0\n",
    "\n",
    "    for (content_image, content_path), (style_image, style_path) in data_loader_eval:\n",
    "        if eval_iteration >= EVAL_ITERATIONS:\n",
    "            break\n",
    "\n",
    "        perceptual_loss = 0.0\n",
    "        style_loss = 0.0\n",
    "        eval_iteration += 1\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            content_image = content_image.to('cuda')\n",
    "            style_image = style_image.to('cuda')\n",
    "\n",
    "        #content_representation = content_encoder(content_image)\n",
    "        #style_representation = style_encoder(style_image)\n",
    "\n",
    "        #transformed_content = function.adain(content_representation, style_representation)\n",
    "        #reconstruction = decoder(transformed_content)\n",
    "        #reconstruction = decoder(content_representation, style_representation)\n",
    "\n",
    "        reconstruction, style_representation = forward(content_image, style_image, content_encoder, style_encoder, decoder)\n",
    "\n",
    "        features_content = loss_net(content_image)\n",
    "        features_style = loss_net(style_image)\n",
    "        features_reconstruction = loss_net(reconstruction)\n",
    "\n",
    "        perceptual_loss += loss.perceptual_loss(features_content, features_reconstruction, CONTENT_LOSS_WEIGHTS)\n",
    "        style_loss += loss.style_loss(features_style, features_reconstruction, STYLE_LOSS_WEIGHTS)\n",
    "\n",
    "        torch.save(content_image.cpu(), f'output/eval_{eval_iteration}_content.pt')\n",
    "        torch.save(style_image.cpu(), f'output/eval_{eval_iteration}_style.pt')\n",
    "        torch.save(reconstruction.cpu(), f'output/eval_{eval_iteration}_reconstruction.pt')\n",
    "\n",
    "        print(f'Evaluation {eval_iteration:02d} : Perceptual loss {perceptual_loss:.4f}\\tStyle loss {style_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(content_loss_history_train[10:], \"r\")\n",
    "plt.plot(style_loss_history_train[10:], \"b\")\n",
    "plt.plot(*zip(*content_loss_history_val), \"r--\")\n",
    "plt.plot(*zip(*style_loss_history_val), \"b--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch = torch.load(\"./output/91_9_reconstruction.pt\")\n",
    "image_batch = data.vgg_normalization_undo(image_batch.numpy())\n",
    "#plt.imshow(transforms.ToPILImage()(image))\n",
    "image_batch = np.transpose(image_batch, (0, 2, 3, 1))\n",
    "plt.imshow(image_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(epoch, iteration):\n",
    "    content_batch = torch.load(\"./output/{}_{}_content.pt\".format(epoch, iteration))\n",
    "    style_batch = torch.load(\"./output/{}_{}_style.pt\".format(epoch, iteration))\n",
    "    reconstruction_batch = torch.load(\"./output/{}_{}_reconstruction.pt\".format(epoch, iteration))\n",
    "    \n",
    "    content_batch = data.vgg_normalization_undo(content_batch.numpy())\n",
    "    style_batch = data.vgg_normalization_undo(style_batch.numpy())\n",
    "    reconstruction_batch = data.vgg_normalization_undo(reconstruction_batch.numpy())\n",
    "    \n",
    "    content_batch = np.transpose(content_batch, (0, 2, 3, 1))\n",
    "    style_batch = np.transpose(style_batch, (0, 2, 3, 1))\n",
    "    reconstruction_batch = np.transpose(reconstruction_batch, (0, 2, 3, 1))\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    fig.add_subplot(1, 3, 1)\n",
    "    plt.imshow(content_batch[0])\n",
    "    \n",
    "    fig.add_subplot(1, 3, 2)\n",
    "    plt.imshow(style_batch[0])\n",
    "    \n",
    "    fig.add_subplot(1, 3, 3)\n",
    "    plt.imshow(reconstruction_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(STYLE_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0, 1.05, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.resize_images_offline(\"../dataset/style\", \"../dataset/style_resized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.resize_images_offline(\"../dataset/debug/style\", \"../dataset/debug/style_resized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
