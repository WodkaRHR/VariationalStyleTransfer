{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data, model.unet, model.autoencoder, loss, function\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torch.utils.tensorboard as tb\n",
    "import torchvision\n",
    "import scipy.stats as stats\n",
    "import pickle\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAE = True\n",
    "\n",
    "prefix = \"vae\"\n",
    "if not VAE:\n",
    "    prefix = \"no_vae\"\n",
    "    \n",
    "date = datetime.datetime.now()\n",
    "timestamp = date.strftime(f\"{prefix}_%d-%b-%Y_%H.%M.%S\")\n",
    "os.makedirs(f\"log/{timestamp}\")\n",
    "tb_writer = tb.SummaryWriter(f\"log/{timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_PORTION = 0.2\n",
    "ITERATIONS = 100001\n",
    "VAL_ITERATIONS = 5\n",
    "VAL_ITERATIONS_OVERFIT = 1\n",
    "RESOLUTION = 96\n",
    "CHANNELS = 3\n",
    "STYLE_DIM = 512\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "LOSS_TYPE = 'l2'\n",
    "\n",
    "CONTENT_LOSS_WEIGHTS = {\n",
    "    'relu_4_2' : 2e-2,\n",
    "}\n",
    "\n",
    "STYLE_LOSS_WEIGHTS = {\n",
    "    'relu_1_1' : 1e3,\n",
    "    'relu_2_1' : 5e3,\n",
    "    'relu_3_1' : 1e3,\n",
    "    'relu_4_1' : 1e3,\n",
    "    'relu_5_1' : 1e3,\n",
    "}\n",
    "\n",
    "STYLE_LOSS_ALPHA = 1.0\n",
    "KLD_LOSS_WEIGHT = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHERRYPICKED_DATASET_100 = True\n",
    "\n",
    "bad_dirs = [\n",
    "    \"Baroque\",\n",
    "    \"Contemporary_Realism\",\n",
    "    \"Early_Renaissance\",\n",
    "    \"High_Renaissance\",\n",
    "    \"Mannerism_Late_Renaissance\",\n",
    "    \"New_Realism\",\n",
    "    \"Northern_Renaissance\",\n",
    "    \"Realism\",\n",
    "    \"Rococo\",\n",
    "    \"Impressionism\",\n",
    "    \"Minimalism\",\n",
    "    \"Pointillism\",\n",
    "    \"Pop_Art\",\n",
    "    \"Romanticism\",\n",
    "    \"Symbolism\"\n",
    "]\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "TRAINING_PORTION_STYLE=128\n",
    "\n",
    "if CHERRYPICKED_DATASET_100:\n",
    "    data_style_train = data.load_dataset(\"../dataset/style_cherrypicked/train\", resolution=RESOLUTION)\n",
    "    data_style_val = data.load_dataset(\"../dataset/style_cherrypicked/validation\", resolution=RESOLUTION)\n",
    "else:\n",
    "    paths = data.list_images(\"../dataset/style\")\n",
    "    filtered_paths = data.filter_images(paths, bad_dirs, \"style_images_no_face.pkl\", \"../dataset/style/wikiart\")\n",
    "\n",
    "    data_style = data.load_dataset_from_list(filtered_paths, resolution=RESOLUTION)\n",
    "    data_style, _ = torch.utils.data.random_split(data_style, [TRAINING_PORTION_STYLE, len(data_style) - TRAINING_PORTION_STYLE])\n",
    "    data_style_train, data_style_val = torch.utils.data.random_split(data_style, [len(data_style) - int(VAL_PORTION * len(data_style)), int(VAL_PORTION * len(data_style))])\n",
    "    \n",
    "data_loader_style_train = DataLoader(data_style_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "data_loader_style_val = DataLoader(data_style_val, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "TRAINING_PORTION=2048\n",
    "data_content = data.load_debug_dataset('../dataset/content', resolution=RESOLUTION)\n",
    "data_content, _ = torch.utils.data.random_split(data_content, [TRAINING_PORTION, len(data_content) - TRAINING_PORTION])\n",
    "data_content_train, data_content_val = torch.utils.data.random_split(data_content, [len(data_content) - int(VAL_PORTION * len(data_content)), int(VAL_PORTION * len(data_content))])\n",
    "data_loader_content_train = DataLoader(data_content_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "data_content_val = data.load_debug_dataset('../dataset/debug/content', resolution=RESOLUTION)\n",
    "data_loader_content_val = DataLoader(data_content_val, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "data_loader_train = data.DatasetPairIterator(data_loader_content_train, data_loader_style_train)\n",
    "data_loader_val = data.DatasetPairIterator(data_loader_content_val, data_loader_style_val)\n",
    "\n",
    "# NO REAL VALIDATION, USES TRAINING STYLE DATA\n",
    "data_loader_val_overfit = data.DatasetPairIterator(data_loader_content_val, data_loader_style_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNUP_CONVOLUTIONS = 5\n",
    "ADAIN_CONVOLUTIONS = 7\n",
    "STYLE_DOWN_CONVOLUTIONS = 5\n",
    "NUM_LAYERS_NO_CONNECTION = 0\n",
    "RESIDUAL_STYLE = True\n",
    "RESIDUAL_DOWN = True\n",
    "RESIDUAL_ADAIN = True\n",
    "RESIDUAL_UP = True\n",
    "STYLE_NORM = True\n",
    "DOWN_NORM = 'in'\n",
    "UP_NORM = 'adain'\n",
    "\n",
    "if not VAE:\n",
    "    STYLE_DIM = STYLE_DIM * 2\n",
    "\n",
    "unet = model.unet.UNetAutoencoder(3, STYLE_DIM, residual_downsampling=RESIDUAL_DOWN, residual_adain=RESIDUAL_ADAIN, residual_upsampling=RESIDUAL_UP, \n",
    "        down_normalization=DOWN_NORM, up_normalization=UP_NORM, num_adain_convolutions=ADAIN_CONVOLUTIONS, \n",
    "        num_downup_convolutions=DOWNUP_CONVOLUTIONS, num_downup_without_connections=NUM_LAYERS_NO_CONNECTION, output_activation='sigmoid')\n",
    "\n",
    "if VAE:\n",
    "    style_encoder = model.autoencoder.Encoder(2 * STYLE_DIM, normalization=STYLE_NORM, residual=RESIDUAL_STYLE, num_down_convolutions=STYLE_DOWN_CONVOLUTIONS)\n",
    "else:\n",
    "    style_encoder = model.autoencoder.Encoder(STYLE_DIM, normalization=STYLE_NORM, residual=RESIDUAL_STYLE, num_down_convolutions=STYLE_DOWN_CONVOLUTIONS)\n",
    "\n",
    "loss_net = loss.LossNet()\n",
    "_ = loss_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): \n",
    "    unet = unet.cuda()\n",
    "    style_encoder = style_encoder.cuda()\n",
    "    loss_net = loss_net.cuda()\n",
    "\n",
    "trainable_parameters = []\n",
    "for parameter in unet.parameters():\n",
    "    trainable_parameters.append(parameter)\n",
    "for parameter in style_encoder.parameters():\n",
    "    trainable_parameters.append(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(trainable_parameters, lr=5e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, verbose=True, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(content_image, style_image):\n",
    "    \"\"\" Forward pass through the architecture.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    content_image : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The content images.\n",
    "    style_image : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The style images.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    stylized : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The stylizations.\n",
    "    style_encoding : torch.Tensor, shape [batch_size, STYLE_DIM]\n",
    "        Style encodings.\n",
    "    style_encoding_mean : torch.Tensor, shape [batch_size, STYLE_DIM]\n",
    "        Means for the style encodings.\n",
    "    style_encoding_logvar : torch.Tensor, shape [batch_size, STYLE_DIM]\n",
    "        Logarithm of the variances of the style encodings.\n",
    "    \"\"\"\n",
    "    style_stats = style_encoder(style_image)\n",
    "    style_mean = style_stats[..., : STYLE_DIM]\n",
    "    style_logvar = style_stats[..., STYLE_DIM : ]\n",
    "    style_sample = function.sample_normal(style_mean, style_logvar)\n",
    "    stylized = unet(content_image, style_sample)\n",
    "    return stylized, style_sample, style_mean, style_logvar\n",
    "\n",
    "def forward_no_vae(content_image, style_image):\n",
    "    \"\"\" Forward pass through the architecture, does not use the variational part.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    content_image : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The content images.\n",
    "    style_image : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The style images.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    stylized : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The stylizations.\n",
    "    style_encoding : torch.Tensor, shape [batch_size, STYLE_DIM]\n",
    "        Style encodings.\n",
    "    \"\"\"\n",
    "    style_encoding = style_encoder(style_image)\n",
    "    stylized = unet(content_image, style_encoding)\n",
    "    return stylized, style_encoding\n",
    "\n",
    "def forward_interpolate(content_image, style_image1, style_image2, interpolation_factor):\n",
    "    \"\"\" Forward pass through the architecture, creates style interpolations.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    content_image : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The content images.\n",
    "    style_image1 : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The first style images.\n",
    "    style_image2 : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The second style images (for interpolation).\n",
    "    interpolation_factor : float\n",
    "        Value between 0 and 1, decides how to weight style_image1 and style_image2.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    stylized : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The stylizations.\n",
    "    style_sample : torch.Tensor, shape [batch_size, STYLE_DIM]\n",
    "        Style encodings.\n",
    "    \"\"\"\n",
    "    style_stats1 = style_encoder(style_image1)\n",
    "    style_mean1 = style_stats1[..., : STYLE_DIM]\n",
    "    style_logvar1 = style_stats1[..., STYLE_DIM : ]\n",
    "    style_sample1 = function.sample_normal(style_mean1, style_logvar1)\n",
    "    \n",
    "    style_stats2 = style_encoder(style_image2)\n",
    "    style_mean2 = style_stats2[..., : STYLE_DIM]\n",
    "    style_logvar2 = style_stats2[..., STYLE_DIM : ]\n",
    "    style_sample2 = function.sample_normal(style_mean2, style_logvar2)\n",
    "    \n",
    "    style_sample = interpolation_factor * style_sample1 + (1 - interpolation_factor) * style_sample2\n",
    "    \n",
    "    stylized = unet(content_image, style_sample)\n",
    "    return stylized, style_sample\n",
    "\n",
    "def forward_sample(content_image):\n",
    "    \"\"\" Forward pass through the architecture, uses sampling to create random styles.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    content_image : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The content images.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    stylized : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The stylizations.\n",
    "    style_sample : torch.Tensor, shape [batch_size, STYLE_DIM]\n",
    "        Style encodings.\n",
    "    \"\"\"\n",
    "    style_sample = torch.randn((BATCH_SIZE, STYLE_DIM), device=content_image.device, requires_grad=False)\n",
    "    stylized = unet(content_image, style_sample)\n",
    "    return stylized, style_sample\n",
    "\n",
    "def forward_sample_from_embedding(content_image, style_mean, style_logvar):\n",
    "    \"\"\" Forward pass through the architecture. Samples a style from a given mean and logvar.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    content_image : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The content images.\n",
    "    style_mean : torch.Tensor, shape [batch_size, STYLE_DIM]\n",
    "        The style embedding mean.\n",
    "    style_logvar : torch.Tensor, shape [batch_size, STYLE_DIM]\n",
    "        The style embedding logvar.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    stylized : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The stylizations.\n",
    "    style_sample : torch.Tensor, shape [batch_size, STYLE_DIM]\n",
    "        Style encodings.\n",
    "    \"\"\"\n",
    "    style_sample = function.sample_normal(style_mean, style_logvar)\n",
    "    stylized = unet(content_image, style_sample)\n",
    "    return stylized, style_sample\n",
    "\n",
    "def forward_interpolate_no_vae(content_image, style_image1, style_image2, interpolation_factor):\n",
    "    \"\"\" Forward pass through the architecture without the variational part, creates style interpolations.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    content_image : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The content images.\n",
    "    style_image1 : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The first style images.\n",
    "    style_image2 : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The second style images (for interpolation).\n",
    "    interpolation_factor : float\n",
    "        Value between 0 and 1, decides how to weight style_image1 and style_image2.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    stylized : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The stylizations.\n",
    "    style_embedding : torch.Tensor, shape [batch_size, STYLE_DIM]\n",
    "        Style encodings.\n",
    "    \"\"\"\n",
    "    style_embedding1 = style_encoder(style_image1)\n",
    "    style_embedding2 = style_encoder(style_image2)\n",
    "    \n",
    "    style_embedding = interpolation_factor * style_embedding1 + (1 - interpolation_factor) * style_embedding2\n",
    "    \n",
    "    stylized = unet(content_image, style_embedding)\n",
    "    return stylized, style_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {}\n",
    "parameters[\"RESOLUTION\"] = RESOLUTION\n",
    "parameters[\"STYLE_DIM\"] = STYLE_DIM\n",
    "parameters[\"BATCH_SIZE\"] = BATCH_SIZE\n",
    "parameters[\"LOSS_TYPE\"] = LOSS_TYPE\n",
    "parameters[\"CONTENT_LOSS_WEIGHTS\"] = CONTENT_LOSS_WEIGHTS\n",
    "parameters[\"STYLE_LOSS_WEIGHTS\"] = STYLE_LOSS_WEIGHTS\n",
    "parameters[\"STYLE_LOSS_ALPHA\"] = STYLE_LOSS_ALPHA\n",
    "parameters[\"KLD_LOSS_WEIGHT\"] = KLD_LOSS_WEIGHT\n",
    "parameters[\"DOWNUP_CONVOLUTIONS\"] = DOWNUP_CONVOLUTIONS\n",
    "parameters[\"ADAIN_CONVOLUTIONS\"] = ADAIN_CONVOLUTIONS\n",
    "parameters[\"STYLE_DOWN_CONVOLUTIONS\"] = STYLE_DOWN_CONVOLUTIONS\n",
    "parameters[\"NUM_LAYERS_NO_CONNECTION\"] = NUM_LAYERS_NO_CONNECTION\n",
    "parameters[\"TRAINING_PORTION_STYLE\"] = TRAINING_PORTION_STYLE\n",
    "parameters[\"TRAINING_PORTION\"] = TRAINING_PORTION\n",
    "parameters[\"VAL_PORTION\"] = VAL_PORTION\n",
    "parameters[\"RESIDUAL_STYLE\"] = RESIDUAL_STYLE\n",
    "parameters[\"RESIDUAL_DOWN\"] = RESIDUAL_DOWN\n",
    "parameters[\"RESIDUAL_ADAIN\"] = RESIDUAL_ADAIN\n",
    "parameters[\"RESIDUAL_UP\"] = RESIDUAL_UP\n",
    "parameters[\"STYLE_NORM\"] = STYLE_NORM\n",
    "parameters[\"DOWN_NORM\"] = DOWN_NORM\n",
    "parameters[\"UP_NORM\"] = UP_NORM\n",
    "parameters[\"CHERRYPICKED_DATASET_100\"] = CHERRYPICKED_DATASET_100\n",
    "parameters[\"VAE\"] = VAE\n",
    "\n",
    "tb_writer.add_text(\"parameters\", str(parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iteration = 0\n",
    "val_step = 0\n",
    "val_step_overfit = 0\n",
    "for (content_image, content_path), (style_image, style_path) in data_loader_train:\n",
    "    if iteration >= ITERATIONS: \n",
    "        break\n",
    "    if torch.cuda.is_available():\n",
    "        content_image = content_image.to('cuda')\n",
    "        style_image = style_image.to('cuda')\n",
    "    \n",
    "    unet.train(), style_encoder.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    stylized, style_encoding, style_mean, style_logvar = forward(content_image, style_image)\n",
    "    \n",
    "    features_content = loss_net(content_image)\n",
    "    features_style = loss_net(style_image)\n",
    "    features_stylized = loss_net(stylized)\n",
    "    \n",
    "    perceptual_loss = loss.perceptual_loss(features_content, features_stylized, CONTENT_LOSS_WEIGHTS, loss=LOSS_TYPE)\n",
    "    style_loss = loss.style_loss(features_style, features_stylized, STYLE_LOSS_WEIGHTS, loss=LOSS_TYPE)\n",
    "    kld_loss = loss.kld_loss(style_mean, style_logvar)\n",
    "    total_loss = perceptual_loss + STYLE_LOSS_ALPHA * style_loss + KLD_LOSS_WEIGHT * kld_loss\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    tb_writer.add_scalar('train loss', total_loss.item(), iteration)\n",
    "    tb_writer.add_scalar('train perceptual loss', perceptual_loss.item(), iteration)\n",
    "    tb_writer.add_scalar('train kld loss', kld_loss.item(), iteration)\n",
    "    tb_writer.add_scalar('train style loss', style_loss.item(), iteration)\n",
    "    print(f'\\r{iteration:5d} / {ITERATIONS}: loss : {total_loss.item():.4f} -- perceptual loss : {perceptual_loss.item():.4f} -- style loss : {style_loss.item():.4f} -- kld loss : {kld_loss.item():.4f}', end='\\r')\n",
    "    \n",
    "    if iteration % 1000 == 0:\n",
    "        torch.save({\n",
    "            'unet_state_dict': unet.state_dict(),\n",
    "            'style_encoder_state_dict': style_encoder.state_dict(),\n",
    "        }, f\"log/{timestamp}/model_{iteration}.pt\")\n",
    "    \n",
    "    if iteration % 100 == 0:\n",
    "        # Validate\n",
    "        print('\\nValidation...')\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            val_iteration = 0\n",
    "            val_iteration_overfit = 0\n",
    "            unet.eval(), style_encoder.eval()\n",
    "            fig = plt.figure(figsize=(20,10))\n",
    "            for (content_image, content_path), (style_image, style_path) in data_loader_val:\n",
    "                \n",
    "                if val_iteration >= VAL_ITERATIONS:\n",
    "                    break\n",
    "                    \n",
    "                if torch.cuda.is_available():\n",
    "                    content_image = content_image.to('cuda')\n",
    "                    style_image = style_image.to('cuda')\n",
    "                \n",
    "                stylized, style_encoding, style_mean, style_logvar = forward(content_image, style_image)\n",
    "    \n",
    "                features_content = loss_net(content_image)\n",
    "                features_style = loss_net(style_image)\n",
    "                features_stylized = loss_net(stylized)\n",
    "\n",
    "                perceptual_loss = loss.perceptual_loss(features_content, features_stylized, CONTENT_LOSS_WEIGHTS, loss=LOSS_TYPE)\n",
    "                style_loss = loss.style_loss(features_style, features_stylized, STYLE_LOSS_WEIGHTS, loss=LOSS_TYPE)\n",
    "                kld_loss = loss.kld_loss(style_mean, style_logvar)\n",
    "                total_loss = perceptual_loss + STYLE_LOSS_ALPHA * style_loss + KLD_LOSS_WEIGHT * kld_loss\n",
    "                total_val_loss += total_loss\n",
    "                \n",
    "                fig.add_subplot(2, VAL_ITERATIONS, val_iteration + 1)\n",
    "                x = np.linspace(-5, 5, 100)\n",
    "                plt.plot(x, stats.norm.pdf(x), color='red', linestyle='dashed')\n",
    "                plt.hist(style_encoding.detach().cpu().numpy().reshape(-1), density=True)\n",
    "                fig.add_subplot(2, VAL_ITERATIONS, VAL_ITERATIONS + val_iteration + 1)\n",
    "                plt.hist(style_encoding.detach().cpu().numpy().std(0), density=True)\n",
    "                \n",
    "                tb_writer.add_scalar('validation loss', total_loss.item(), val_step)\n",
    "                tb_writer.add_scalar('validation perceptual loss', perceptual_loss.item(), val_step)\n",
    "                tb_writer.add_scalar('validation style loss', style_loss.item(), val_step)\n",
    "                tb_writer.add_scalar('validation kld loss', kld_loss.item(), val_step)\n",
    "                tb_writer.add_images('validation images', torch.from_numpy(np.concatenate([\n",
    "                    img.detach().cpu().numpy() for img in [content_image, style_image, stylized]\n",
    "                ])), val_step)\n",
    "                val_iteration += 1\n",
    "                val_step += 1\n",
    "                print(f'\\r{val_iteration:5d} / {VAL_ITERATIONS}: loss : {total_loss.item():.4f} -- perceptual loss : {perceptual_loss.item():.4f} -- style loss : {style_loss.item():.4f} -- kld loss : {kld_loss.item():.4f}', end='\\r')\n",
    "            \n",
    "            # generate stylization from training styles\n",
    "            for (content_image, content_path), (style_image, style_path) in data_loader_val_overfit:\n",
    "                if val_iteration_overfit >= VAL_ITERATIONS_OVERFIT:\n",
    "                    break\n",
    "                    \n",
    "                if torch.cuda.is_available():\n",
    "                    content_image = content_image.to('cuda')\n",
    "                    style_image = style_image.to('cuda')\n",
    "                \n",
    "                stylized, style_encoding, style_mean, style_logvar = forward(content_image, style_image)\n",
    "    \n",
    "                features_content = loss_net(content_image)\n",
    "                features_style = loss_net(style_image)\n",
    "                features_stylized = loss_net(stylized)\n",
    "                \n",
    "                tb_writer.add_images('overfit images', torch.from_numpy(np.concatenate([\n",
    "                    img.detach().cpu().numpy() for img in [content_image, style_image, stylized]\n",
    "                ])), val_step)\n",
    "                val_iteration_overfit += 1\n",
    "                val_step_overfit += 1\n",
    "            \n",
    "            plt.show()\n",
    "            total_val_loss /= VAL_ITERATIONS\n",
    "            print(f'\\nAverage val loss: {total_val_loss}')\n",
    "            lr_scheduler.step(total_val_loss)\n",
    "            print(f'Training with lr {optimizer.param_groups[0][\"lr\"]}...')\n",
    "            \n",
    "            \n",
    "    iteration += 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# The following part of the Notebook is for evaluation and to create visualizations of the results\n",
    "\n",
    "\n",
    "# Load previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_PATH = \"log_alphas/alpha_1_vae_25-Jan-2020_16.40.00/model_29000.pt\"\n",
    "\n",
    "DOWNUP_CONVOLUTIONS = 5 #3\n",
    "ADAIN_CONVOLUTIONS = 7 #3\n",
    "STYLE_DOWN_CONVOLUTIONS = 5 #3\n",
    "NUM_LAYERS_NO_CONNECTION = 0\n",
    "RESIDUAL_STYLE = True # False\n",
    "RESIDUAL_DOWN = True # False\n",
    "RESIDUAL_ADAIN = True\n",
    "RESIDUAL_UP = True\n",
    "STYLE_NORM = True\n",
    "DOWN_NORM = 'in'\n",
    "UP_NORM = 'adain'\n",
    "\n",
    "VAE = True\n",
    "\n",
    "unet = model.unet.UNetAutoencoder(3, STYLE_DIM, residual_downsampling=RESIDUAL_DOWN, residual_adain=RESIDUAL_ADAIN, residual_upsampling=RESIDUAL_UP, \n",
    "        down_normalization=DOWN_NORM, up_normalization=UP_NORM, num_adain_convolutions=ADAIN_CONVOLUTIONS, \n",
    "        num_downup_convolutions=DOWNUP_CONVOLUTIONS, num_downup_without_connections=NUM_LAYERS_NO_CONNECTION, output_activation='sigmoid')\n",
    "\n",
    "if VAE:\n",
    "    style_encoder = model.autoencoder.Encoder(2 * STYLE_DIM, normalization=STYLE_NORM, residual=RESIDUAL_STYLE, num_down_convolutions=STYLE_DOWN_CONVOLUTIONS)\n",
    "else:\n",
    "    style_encoder = model.autoencoder.Encoder(STYLE_DIM, normalization=STYLE_NORM, residual=RESIDUAL_STYLE, num_down_convolutions=STYLE_DOWN_CONVOLUTIONS)\n",
    "\n",
    "loss_net = loss.LossNet()\n",
    "_ = loss_net.eval()\n",
    "\n",
    "checkpoint = torch.load(LOAD_PATH)\n",
    "unet.load_state_dict(checkpoint[\"unet_state_dict\"])\n",
    "style_encoder.load_state_dict(checkpoint[\"style_encoder_state_dict\"])\n",
    "\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    "    unet = unet.cuda()\n",
    "    style_encoder = style_encoder.cuda()\n",
    "    loss_net = loss_net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling random styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_ITERATIONS = 100\n",
    "with torch.no_grad():\n",
    "    val_iteration = 0\n",
    "    val_iteration_overfit = 0\n",
    "    unet.eval(), style_encoder.eval()\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    for (content_image, content_path), (style_image, style_path) in data_loader_val:\n",
    "\n",
    "        if val_iteration >= SAMPLING_ITERATIONS:\n",
    "            break\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            content_image = content_image.to('cuda')\n",
    "            style_image = style_image.to('cuda')\n",
    "\n",
    "        stylized, style_encoding = forward_sample(content_image)\n",
    "\n",
    "        features_content = loss_net(content_image)\n",
    "        features_style = loss_net(style_image)\n",
    "        features_stylized = loss_net(stylized)\n",
    "\n",
    "        tb_writer.add_images('randomly sampled styles', torch.from_numpy(np.concatenate([\n",
    "            img.detach().cpu().numpy() for img in [content_image, stylized]\n",
    "        ])), val_iteration)\n",
    "        val_iteration += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERPOLATION_ITERATIONS = 30\n",
    "INTERPOLOATION_FACTORS = [1.0, .75, .5, .25, 0.0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_iteration = 0\n",
    "    unet.eval(), style_encoder.eval()\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    for (content_image, content_path), (style_image1, style_path) in data_loader_val:\n",
    "\n",
    "        _, (style_image2, style_path) = next(iter(data_loader_val))\n",
    "        \n",
    "        if val_iteration >= INTERPOLATION_ITERATIONS:\n",
    "            break\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            content_image = content_image.to('cuda')\n",
    "            style_image1 = style_image1.to('cuda')\n",
    "            style_image2 = style_image2.to('cuda')\n",
    "        \n",
    "        interpolations = []\n",
    "        for interpolation in INTERPOLOATION_FACTORS:\n",
    "            stylized, style_encoding = forward_interpolate(content_image, style_image1, style_image2, interpolation)\n",
    "            interpolations.append(stylized[:8])\n",
    "        \n",
    "        img_list = [content_image[:8], style_image1[:8]] + interpolations + [style_image2[:8]]\n",
    "        \n",
    "        tb_writer.add_images('interpolation images validation', torch.from_numpy(np.concatenate([\n",
    "            img.detach().cpu().numpy() for img in img_list\n",
    "        ])), val_iteration)\n",
    "        val_iteration += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style interpolation overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERPOLATION_ITERATIONS = 30\n",
    "INTERPOLOATION_FACTORS = [1.0, .75, .5, .25, 0.0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_iteration = 0\n",
    "    unet.eval(), style_encoder.eval()\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    for (content_image, content_path), (style_image1, style_path) in data_loader_val_overfit:\n",
    "\n",
    "        _, (style_image2, style_path) = next(iter(data_loader_val_overfit))\n",
    "        \n",
    "        if val_iteration >= INTERPOLATION_ITERATIONS:\n",
    "            break\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            content_image = content_image.to('cuda')\n",
    "            style_image1 = style_image1.to('cuda')\n",
    "            style_image2 = style_image2.to('cuda')\n",
    "        \n",
    "        interpolations = []\n",
    "        for interpolation in INTERPOLOATION_FACTORS:\n",
    "            stylized, style_encoding = forward_interpolate(content_image, style_image1, style_image2, interpolation)\n",
    "            interpolations.append(stylized[:8])\n",
    "        \n",
    "        img_list = [content_image[:8], style_image1[:8]] + interpolations + [style_image2[:8]]\n",
    "        \n",
    "        tb_writer.add_images('interpolation images overfit', torch.from_numpy(np.concatenate([\n",
    "            img.detach().cpu().numpy() for img in img_list\n",
    "        ])), val_iteration)\n",
    "        val_iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training without VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters['VAE'] = False\n",
    "\n",
    "iteration = 0\n",
    "val_step = 0\n",
    "val_step_overfit = 0\n",
    "for (content_image, content_path), (style_image, style_path) in data_loader_train:\n",
    "    if iteration >= ITERATIONS: \n",
    "        break\n",
    "    if torch.cuda.is_available():\n",
    "        content_image = content_image.to('cuda')\n",
    "        style_image = style_image.to('cuda')\n",
    "    \n",
    "    unet.train(), style_encoder.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    stylized, style_encoding = forward_no_vae(content_image, style_image)\n",
    "    \n",
    "    features_content = loss_net(content_image)\n",
    "    features_style = loss_net(style_image)\n",
    "    features_stylized = loss_net(stylized)\n",
    "    \n",
    "    perceptual_loss = loss.perceptual_loss(features_content, features_stylized, CONTENT_LOSS_WEIGHTS, loss=LOSS_TYPE)\n",
    "    style_loss = loss.style_loss(features_style, features_stylized, STYLE_LOSS_WEIGHTS, loss=LOSS_TYPE)\n",
    "    total_loss = perceptual_loss + STYLE_LOSS_ALPHA * style_loss\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    tb_writer.add_scalar('train loss', total_loss.item(), iteration)\n",
    "    tb_writer.add_scalar('train perceptual loss', perceptual_loss.item(), iteration)\n",
    "    tb_writer.add_scalar('train style loss', style_loss.item(), iteration)\n",
    "    print(f'\\r{iteration:5d} / {ITERATIONS}: loss : {total_loss.item():.4f} -- perceptual loss : {perceptual_loss.item():.4f} -- style loss : {style_loss.item():.4f}', end='\\r')\n",
    "    \n",
    "    if iteration % 1000 == 0:\n",
    "        torch.save({\n",
    "            'unet_state_dict': unet.state_dict(),\n",
    "            'style_encoder_state_dict': style_encoder.state_dict(),\n",
    "        }, f\"log/{timestamp}/model_{iteration}.pt\")\n",
    "    \n",
    "    if iteration % 100 == 0:\n",
    "        # Validate\n",
    "        print('\\nValidation...')\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            val_iteration = 0\n",
    "            val_iteration_overfit = 0\n",
    "            unet.eval(), style_encoder.eval()\n",
    "            fig = plt.figure(figsize=(20,10))\n",
    "            for (content_image, content_path), (style_image, style_path) in data_loader_val:\n",
    "                \n",
    "                if val_iteration >= VAL_ITERATIONS:\n",
    "                    break\n",
    "                    \n",
    "                if torch.cuda.is_available():\n",
    "                    content_image = content_image.to('cuda')\n",
    "                    style_image = style_image.to('cuda')\n",
    "                \n",
    "                stylized, style_encoding = forward_no_vae(content_image, style_image)\n",
    "    \n",
    "                features_content = loss_net(content_image)\n",
    "                features_style = loss_net(style_image)\n",
    "                features_stylized = loss_net(stylized)\n",
    "\n",
    "                perceptual_loss = loss.perceptual_loss(features_content, features_stylized, CONTENT_LOSS_WEIGHTS, loss=LOSS_TYPE)\n",
    "                style_loss = loss.style_loss(features_style, features_stylized, STYLE_LOSS_WEIGHTS, loss=LOSS_TYPE)\n",
    "                total_loss = perceptual_loss + STYLE_LOSS_ALPHA * style_loss\n",
    "                total_val_loss += total_loss\n",
    "                \n",
    "                fig.add_subplot(2, VAL_ITERATIONS, val_iteration + 1)\n",
    "                x = np.linspace(-5, 5, 100)\n",
    "                plt.plot(x, stats.norm.pdf(x), color='red', linestyle='dashed')\n",
    "                plt.hist(style_encoding.detach().cpu().numpy().reshape(-1), density=True)\n",
    "                fig.add_subplot(2, VAL_ITERATIONS, VAL_ITERATIONS + val_iteration + 1)\n",
    "                plt.hist(style_encoding.detach().cpu().numpy().std(0), density=True)\n",
    "                \n",
    "                tb_writer.add_scalar('validation loss', total_loss.item(), val_step)\n",
    "                tb_writer.add_scalar('validation perceptual loss', perceptual_loss.item(), val_step)\n",
    "                tb_writer.add_scalar('validation style loss', style_loss.item(), val_step)\n",
    "                tb_writer.add_images('validation images', torch.from_numpy(np.concatenate([\n",
    "                    img.detach().cpu().numpy() for img in [content_image, style_image, stylized]\n",
    "                ])), val_step)\n",
    "                val_iteration += 1\n",
    "                val_step += 1\n",
    "                print(f'\\r{val_iteration:5d} / {VAL_ITERATIONS}: loss : {total_loss.item():.4f} -- perceptual loss : {perceptual_loss.item():.4f} -- style loss : {style_loss.item():.4f}', end='\\r')\n",
    "            \n",
    "            for (content_image, content_path), (style_image, style_path) in data_loader_val_overfit:\n",
    "                \n",
    "                if val_iteration_overfit >= VAL_ITERATIONS_OVERFIT:\n",
    "                    break\n",
    "                    \n",
    "                if torch.cuda.is_available():\n",
    "                    content_image = content_image.to('cuda')\n",
    "                    style_image = style_image.to('cuda')\n",
    "                \n",
    "                stylized, style_encoding = forward_no_vae(content_image, style_image)\n",
    "    \n",
    "                features_content = loss_net(content_image)\n",
    "                features_style = loss_net(style_image)\n",
    "                features_stylized = loss_net(stylized)\n",
    "                \n",
    "                tb_writer.add_images('overfit images', torch.from_numpy(np.concatenate([\n",
    "                    img.detach().cpu().numpy() for img in [content_image, style_image, stylized]\n",
    "                ])), val_step)\n",
    "                val_iteration_overfit += 1\n",
    "                val_step_overfit += 1\n",
    "            \n",
    "            plt.show()\n",
    "            total_val_loss /= VAL_ITERATIONS\n",
    "            print(f'\\nAverage val loss: {total_val_loss}')\n",
    "            lr_scheduler.step(total_val_loss)\n",
    "            print(f'Training with lr {optimizer.param_groups[0][\"lr\"]}...')\n",
    "            \n",
    "            \n",
    "    iteration += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style interpolation overfit no VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERPOLATION_ITERATIONS = 30\n",
    "INTERPOLOATION_FACTORS = [1.0, .75, .5, .25, 0.0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_iteration = 0\n",
    "    unet.eval(), style_encoder.eval()\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    for (content_image, content_path), (style_image1, style_path) in data_loader_val_overfit:\n",
    "\n",
    "        _, (style_image2, style_path) = next(iter(data_loader_val_overfit))\n",
    "        \n",
    "        if val_iteration >= INTERPOLATION_ITERATIONS:\n",
    "            break\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            content_image = content_image.to('cuda')\n",
    "            style_image1 = style_image1.to('cuda')\n",
    "            style_image2 = style_image2.to('cuda')\n",
    "        \n",
    "        interpolations = []\n",
    "        for interpolation in INTERPOLOATION_FACTORS:\n",
    "            stylized, style_encoding = forward_interpolate_no_vae(content_image, style_image1, style_image2, interpolation)\n",
    "            interpolations.append(stylized[:8])\n",
    "        \n",
    "        img_list = [content_image[:8], style_image1[:8]] + interpolations + [style_image2[:8]]\n",
    "        \n",
    "        tb_writer.add_images('interpolation images overfit', torch.from_numpy(np.concatenate([\n",
    "            img.detach().cpu().numpy() for img in img_list\n",
    "        ])), val_iteration)\n",
    "        val_iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create survey images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataloader\n",
    "data_content_test = data.load_debug_dataset('../dataset/test/survey', resolution=RESOLUTION)\n",
    "data_loader_content_test = DataLoader(data_content_test, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "data_style_test = data.load_debug_dataset('../dataset/style_cherrypicked/test/', resolution=RESOLUTION)\n",
    "data_loader_style_test = DataLoader(data_style_test, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "# dataloader with test content and training style\n",
    "data_loader_test_overfit = data.DatasetPairIterator(data_loader_content_test, data_loader_style_train)\n",
    "\n",
    "data_loader_test = data.DatasetPairIterator(data_loader_content_test, data_loader_style_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_ITERATIONS=100\n",
    "\n",
    "# create images\n",
    "with torch.no_grad():\n",
    "    test_iteration = 0\n",
    "    unet.eval(), style_encoder.eval()\n",
    "    for (content_image, content_path), (style_image, style_path) in data_loader_test_overfit:\n",
    "                \n",
    "        if test_iteration >= TEST_ITERATIONS:\n",
    "            break\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            content_image = content_image.to('cuda')\n",
    "            style_image = style_image.to('cuda')\n",
    "\n",
    "        stylized, style_encoding, style_mean, style_logvar = forward(content_image, style_image)\n",
    "\n",
    "        features_content = loss_net(content_image)\n",
    "        features_style = loss_net(style_image)\n",
    "        features_stylized = loss_net(stylized)\n",
    "\n",
    "        tb_writer.add_images('test images', torch.from_numpy(np.concatenate([\n",
    "            img.detach().cpu().numpy() for img in [content_image, style_image, stylized]\n",
    "        ])), test_iteration)\n",
    "        test_iteration += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_ITERATIONS=100\n",
    "\n",
    "# create images\n",
    "with torch.no_grad():\n",
    "    test_iteration = 0\n",
    "    unet.eval(), style_encoder.eval()\n",
    "    for (content_image, content_path), (style_image, style_path) in data_loader_test:\n",
    "                \n",
    "        if test_iteration >= TEST_ITERATIONS:\n",
    "            break\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            content_image = content_image.to('cuda')\n",
    "            style_image = style_image.to('cuda')\n",
    "\n",
    "        stylized, style_encoding, style_mean, style_logvar = forward(content_image, style_image)\n",
    "\n",
    "        features_content = loss_net(content_image)\n",
    "        features_style = loss_net(style_image)\n",
    "        features_stylized = loss_net(stylized)\n",
    "\n",
    "        tb_writer.add_images('test images, test styles', torch.from_numpy(np.concatenate([\n",
    "            img.detach().cpu().numpy() for img in [content_image, style_image, stylized]\n",
    "        ])), test_iteration)\n",
    "        test_iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style interpolation test content, training style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERPOLATION_ITERATIONS = 50\n",
    "INTERPOLOATION_FACTORS = [1.0, .75, .5, .25, 0.0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_iteration = 0\n",
    "    unet.eval(), style_encoder.eval()\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    for (content_image, content_path), (style_image1, style_path) in data_loader_test_overfit:\n",
    "\n",
    "        _, (style_image2, style_path) = next(iter(data_loader_test_overfit))\n",
    "        \n",
    "        if val_iteration >= INTERPOLATION_ITERATIONS:\n",
    "            break\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            content_image = content_image.to('cuda')\n",
    "            style_image1 = style_image1.to('cuda')\n",
    "            style_image2 = style_image2.to('cuda')\n",
    "        \n",
    "        interpolations = []\n",
    "        for interpolation in INTERPOLOATION_FACTORS:\n",
    "            stylized, style_encoding = forward_interpolate(content_image, style_image1, style_image2, interpolation)\n",
    "            interpolations.append(stylized)\n",
    "            \n",
    "        interpolations = torch.stack(interpolations, dim=1).unsqueeze(2)\n",
    "        img_list = [torch.cat([content_image[i].unsqueeze(0), style_image1[i].unsqueeze(0), *interpolations[i], style_image2[i].unsqueeze(0)], dim=0) for i in range(BATCH_SIZE)]\n",
    "        \n",
    "        interpolation_tensor = torch.cat(img_list, dim=0)\n",
    "        \n",
    "        grid = torchvision.utils.make_grid(interpolation_tensor, nrow=8)\n",
    "        tb_writer.add_image(\"vae style interpolation grid\", grid, val_iteration)\n",
    "        \n",
    "        val_iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style interpolation test content, test styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERPOLATION_ITERATIONS = 100\n",
    "INTERPOLOATION_FACTORS = [1.0, .75, .5, .25, 0.0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_iteration = 0\n",
    "    unet.eval(), style_encoder.eval()\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    for (content_image, content_path), (style_image1, style_path) in data_loader_test:\n",
    "\n",
    "        _, (style_image2, style_path) = next(iter(data_loader_test))\n",
    "        \n",
    "        if val_iteration >= INTERPOLATION_ITERATIONS:\n",
    "            break\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            content_image = content_image.to('cuda')\n",
    "            style_image1 = style_image1.to('cuda')\n",
    "            style_image2 = style_image2.to('cuda')\n",
    "        \n",
    "        interpolations = []\n",
    "        for interpolation in INTERPOLOATION_FACTORS:\n",
    "            stylized, style_encoding = forward_interpolate(content_image, style_image1, style_image2, interpolation)\n",
    "            interpolations.append(stylized[:8])\n",
    "        \n",
    "        img_list = [content_image[:8], style_image1[:8]] + interpolations + [style_image2[:8]]\n",
    "        \n",
    "        tb_writer.add_images('interpolation images test content, test style', torch.from_numpy(np.concatenate([\n",
    "            img.detach().cpu().numpy() for img in img_list\n",
    "        ])), val_iteration)\n",
    "        val_iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survey images no VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_ITERATIONS=100\n",
    "\n",
    "# test dataloader\n",
    "data_content_test = data.load_debug_dataset('../dataset/test/survey', resolution=RESOLUTION)\n",
    "data_loader_content_test = DataLoader(data_content_test, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "# dataloader with test content and training style\n",
    "data_loader_test_overfit = data.DatasetPairIterator(data_loader_content_test, data_loader_style_train)\n",
    "\n",
    "# create images\n",
    "with torch.no_grad():\n",
    "    test_iteration = 0\n",
    "    unet.eval(), style_encoder.eval()\n",
    "    for (content_image, content_path), (style_image, style_path) in data_loader_test_overfit:\n",
    "                \n",
    "        if test_iteration >= TEST_ITERATIONS:\n",
    "            break\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            content_image = content_image.to('cuda')\n",
    "            style_image = style_image.to('cuda')\n",
    "\n",
    "        stylized, style_encoding = forward_no_vae(content_image, style_image)\n",
    "\n",
    "        features_content = loss_net(content_image)\n",
    "        features_style = loss_net(style_image)\n",
    "        features_stylized = loss_net(stylized)\n",
    "\n",
    "        tb_writer.add_images('test images no vae', torch.from_numpy(np.concatenate([\n",
    "            img.detach().cpu().numpy() for img in [content_image, style_image, stylized]\n",
    "        ])), test_iteration)\n",
    "        test_iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling from style embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_style_debug = data.load_debug_dataset('../dataset/debug/style_test3', resolution=RESOLUTION)\n",
    "data_loader_style_debug = DataLoader(data_style_debug, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "data_loader_debug = data.DatasetPairIterator(data_loader_content_test, data_loader_style_debug)\n",
    "\n",
    "TEST_ITERATIONS=1\n",
    "SAMPLES=16\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "# create images\n",
    "with torch.no_grad():\n",
    "    test_iteration = 0\n",
    "    unet.eval(), style_encoder.eval()\n",
    "    for (content_image, content_path), (style_image, style_path) in data_loader_debug:\n",
    "                \n",
    "        if test_iteration >= TEST_ITERATIONS:\n",
    "            break\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            content_image = content_image.to('cuda')\n",
    "            style_image = style_image.to('cuda')\n",
    "\n",
    "        stylized, style_encoding, style_mean, style_logvar = forward(content_image, style_image)\n",
    "        \n",
    "        stylized_samples = []\n",
    "        for s in range(SAMPLES):\n",
    "            stylized_sample, _ = forward_sample_from_embedding(content_image, style_mean, style_logvar)\n",
    "            stylized_samples.append(stylized_sample)\n",
    "            \n",
    "        stylized_samples = torch.cat(stylized_samples, dim=0)\n",
    "\n",
    "        grid = torchvision.utils.make_grid(torch.cat([content_image, style_image, stylized, stylized_samples], dim=0), nrow=16)\n",
    "        \n",
    "        tb_writer.add_image('samples from style embedding', grid, test_iteration)\n",
    "        test_iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
