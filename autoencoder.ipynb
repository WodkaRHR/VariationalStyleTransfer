{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import data, model, loss\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.tensorboard as tb\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_PORTION = 0.2\n",
    "ITERATIONS = 5000\n",
    "VAL_ITERATIONS = 10\n",
    "RESOLUTION = 32\n",
    "CONTENT_DIM = 512\n",
    "STYLE_DIM = 512\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "CONTENT_LOSS_WEIGHTS = {\n",
    "    'relu_4_2' : 1e0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_style = data.load_debug_dataset('../dataset/debug/style', resolution=RESOLUTION, number_instances=200)\n",
    "data_style_train, data_style_val = torch.utils.data.random_split(data_style, [len(data_style) - int(VAL_PORTION * len(data_style)), int(VAL_PORTION * len(data_style))])\n",
    "data_loader_style_train = DataLoader(data_style_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "data_loader_style_val = DataLoader(data_style_val, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "data_content = data.load_debug_dataset('../dataset/debug/content', resolution=RESOLUTION, number_instances=200)\n",
    "data_content_train, data_content_val = torch.utils.data.random_split(data_content, [len(data_content) - int(VAL_PORTION * len(data_content)), int(VAL_PORTION * len(data_content))])\n",
    "data_loader_content_train = DataLoader(data_content_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "data_loader_content_val = DataLoader(data_content_val, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "data_loader_train = data.DatasetPairIterator(data_loader_content_train, data_loader_style_train)\n",
    "data_loader_val = data.DatasetPairIterator(data_loader_content_val, data_loader_style_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_style = data.load_dataset('../dataset/style', resolution=RESOLUTION)\n",
    "data_style_train, data_style_val = torch.utils.data.random_split(data_style, [len(data_style) - int(VAL_PORTION * len(data_style)), int(VAL_PORTION * len(data_style))])\n",
    "data_loader_style_train = DataLoader(data_style_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "data_loader_style_val = DataLoader(data_style_val, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "data_content = data.load_dataset('../dataset/content', resolution=RESOLUTION)\n",
    "data_content_train, data_content_val = torch.utils.data.random_split(data_content, [len(data_content) - int(VAL_PORTION * len(data_content)), int(VAL_PORTION * len(data_content))])\n",
    "data_loader_content_train = DataLoader(data_content_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "data_loader_content_val = DataLoader(data_content_val, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "data_loader_train = data.DatasetPairIterator(data_loader_content_train, data_loader_style_train)\n",
    "data_loader_val = data.DatasetPairIterator(data_loader_content_val, data_loader_style_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_encoder = model.Encoder(512, normalization=True, residual=True, num_down_convolutions=4)\n",
    "decoder = model.Decoder(512, None, (32, 32), out_channels=3, residual=True, normalization='in', num_up_convolutions=4)\n",
    "\n",
    "#content_encoder = model.ResNetEncoder(CONTENT_DIM, architecture=torchvision.models.resnet18, pretrained=True)\n",
    "#content_encoder = model.VGGEncoder((3, RESOLUTION, RESOLUTION), flattened_output_dim=CONTENT_DIM, architecture=torchvision.models.vgg19, n_layers=19)\n",
    "#decoder = model.Decoder(None, None, (RESOLUTION, RESOLUTION))\n",
    "#decoder = model.VGGDecoder((3, RESOLUTION, RESOLUTION), None, CONTENT_DIM, architecture=torchvision.models.vgg19, n_layers=19)\n",
    "loss_net = loss.LossNet()\n",
    "_ = loss_net.eval()\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content encoder has 6500928 parameters.\n",
      "Decoder has 4926234 parameters.\n"
     ]
    }
   ],
   "source": [
    "print(f'Content encoder has {sum(p.numel() for p in content_encoder.parameters() if p.requires_grad)} parameters.')\n",
    "print(f'Decoder has {sum(p.numel() for p in decoder.parameters() if p.requires_grad)} parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): \n",
    "    content_encoder = content_encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "    loss_net = loss_net.cuda()\n",
    "\n",
    "trainable_parameters = []\n",
    "#linear = torch.nn.Sequential(\n",
    "#    torch.nn.Linear(Cd * Hd * Wd, CONTENT_DIM),\n",
    "#).cuda()\n",
    "for parameter in content_encoder.parameters():\n",
    "    trainable_parameters.append(parameter)\n",
    "for parameter in decoder.parameters():\n",
    "    trainable_parameters.append(parameter)\n",
    "#for parameter in linear.parameters():\n",
    "#    trainable_parameters.append(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(trainable_parameters, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_writer = tb.SummaryWriter('log/autoencoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 / 5000: loss : 36.5201 -- z_mean : 0.06726 -- z_std : 0.14791\n",
      "Validation...\n",
      "\\Training...loss : 34.0379 -- z_mean : 0.07034 -- z_std : 0.14780\n",
      "  100 / 5000: loss : 18.9468 -- z_mean : 0.07868 -- z_std : 0.14941\n",
      "Validation...\n",
      "\\Training...loss : 24.7783 -- z_mean : 0.07850 -- z_std : 0.14218\n",
      "  200 / 5000: loss : 14.2019 -- z_mean : 0.06630 -- z_std : 0.16028\n",
      "Validation...\n",
      "\\Training...loss : 24.1431 -- z_mean : 0.06696 -- z_std : 0.14383\n",
      "  300 / 5000: loss : 10.1863 -- z_mean : 0.05668 -- z_std : 0.17147\n",
      "Validation...\n",
      "\\Training...loss : 27.2567 -- z_mean : 0.05536 -- z_std : 0.14547\n",
      "  400 / 5000: loss : 7.3516 -- z_mean : 0.04749 -- z_std : 0.177941\n",
      "Validation...\n",
      "\\Training...loss : 26.4079 -- z_mean : 0.04538 -- z_std : 0.14708\n",
      "  500 / 5000: loss : 5.6895 -- z_mean : 0.03992 -- z_std : 0.18639\n",
      "Validation...\n",
      "\\Training...loss : 27.1184 -- z_mean : 0.03935 -- z_std : 0.14811\n",
      "  600 / 5000: loss : 4.7365 -- z_mean : 0.04106 -- z_std : 0.18961\n",
      "Validation...\n",
      "\\Training...loss : 27.8698 -- z_mean : 0.04058 -- z_std : 0.14812\n",
      "  700 / 5000: loss : 3.6768 -- z_mean : 0.04180 -- z_std : 0.19106\n",
      "Validation...\n",
      "\\Training...loss : 27.6909 -- z_mean : 0.03800 -- z_std : 0.15014\n",
      "  800 / 5000: loss : 3.8351 -- z_mean : 0.03897 -- z_std : 0.19537\n",
      "Validation...\n",
      "\\Training...loss : 28.7053 -- z_mean : 0.03827 -- z_std : 0.14880\n",
      "  849 / 5000: loss : 3.6207 -- z_mean : 0.03837 -- z_std : 0.19767\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-0fee91528dde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mfeatures_decoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperceptual_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_decoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONTENT_LOSS_WEIGHTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with torch.autograd.set_detect_anomaly(True):\n",
    "    iteration = 0\n",
    "    val_step = 0\n",
    "    for (content_image, content_path), _ in data_loader_train:\n",
    "        if iteration >= ITERATIONS: \n",
    "            break\n",
    "        if torch.cuda.is_available():\n",
    "            content_image = content_image.to('cuda')\n",
    "\n",
    "        content_encoder.train(), decoder.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        z = content_encoder(content_image)\n",
    "        #z = linear(z.view(-1, Cd * Hd * Wd))\n",
    "        decoded = decoder(z, None)\n",
    "\n",
    "        features_content = loss_net(content_image)\n",
    "        features_decoded = loss_net(decoded)\n",
    "        total_loss = loss.perceptual_loss(features_content, features_decoded, CONTENT_LOSS_WEIGHTS)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        z = z.detach().cpu().numpy()\n",
    "\n",
    "        tb_writer.add_scalar('train loss', total_loss.item(), iteration)\n",
    "        print(f'\\r{iteration:5d} / {ITERATIONS}: loss : {total_loss.item():.4f} -- z_mean : {z.mean():.5f} -- z_std : {z.std(axis=0).mean():.5f}', end='\\r')\n",
    "\n",
    "        if iteration % 100 == 0:\n",
    "            tb_writer.add_images('train images', torch.from_numpy(np.concatenate([\n",
    "                data.vgg_normalization_undo(img.detach().cpu().numpy()) for img in [content_image, decoded]\n",
    "            ])), iteration)\n",
    "            # Validate\n",
    "            print('\\nValidation...')\n",
    "            with torch.no_grad():\n",
    "                val_iteration = 0\n",
    "                content_encoder.eval(), decoder.eval()\n",
    "                for (content_image, content_path), _ in data_loader_val:\n",
    "                    if val_iteration >= VAL_ITERATIONS:\n",
    "                        print('\\Training...')\n",
    "                        break\n",
    "\n",
    "                    if torch.cuda.is_available():\n",
    "                        content_image = content_image.to('cuda')\n",
    "\n",
    "\n",
    "                    z = content_encoder(content_image)\n",
    "                    #z = linear(z.view(-1, Cd * Hd * Wd))\n",
    "                    decoded = decoder(z, None)\n",
    "\n",
    "                    features_content = loss_net(content_image)\n",
    "                    features_decoded = loss_net(decoded)\n",
    "                    total_loss = loss.perceptual_loss(features_content, features_decoded, CONTENT_LOSS_WEIGHTS)\n",
    "                    #total_loss = criterion(content_image, decoded)\n",
    "                    \n",
    "                    z = z.detach().cpu().numpy()\n",
    "\n",
    "                    print(f'\\r{val_iteration:5d} / {VAL_ITERATIONS}: loss : {total_loss.item():.4f} -- z_mean : {z.mean():.5f} -- z_std : {z.std(axis=0).mean():.5f}', end='\\r')\n",
    "\n",
    "\n",
    "                    tb_writer.add_scalar('validation loss', total_loss.item(), val_step)\n",
    "                    tb_writer.add_images('validation images', torch.from_numpy(np.concatenate([\n",
    "                        data.vgg_normalization_undo(img.detach().cpu().numpy()) for img in [content_image, decoded]\n",
    "                    ])), val_step)\n",
    "                    val_iteration += 1\n",
    "                    val_step += 1\n",
    "\n",
    "        iteration += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(content_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
