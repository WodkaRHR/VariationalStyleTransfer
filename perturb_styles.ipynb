{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import data, model.unet, model.autoencoder, loss, function\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils\n",
    "import numpy as np\n",
    "import torch.utils.tensorboard as tb\n",
    "import torchvision\n",
    "import scipy.stats as stats\n",
    "import pickle\n",
    "import datetime\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from PIL import Image\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.datetime.now()\n",
    "timestamp = date.strftime(f\"{prefix}_%d-%b-%Y_%H.%M.%S\")\n",
    "os.makedirs(f\"log/{timestamp}\")\n",
    "tb_writer = tb.SummaryWriter(f\"log/{timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_PORTION = 0.2\n",
    "ITERATIONS = 100001\n",
    "VAL_ITERATIONS = 5\n",
    "VAL_ITERATIONS_OVERFIT = 1\n",
    "RESOLUTION = 96\n",
    "CHANNELS = 3\n",
    "STYLE_DIM = 512\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "LOSS_TYPE = 'l2'\n",
    "\n",
    "CONTENT_LOSS_WEIGHTS = {\n",
    "    #'relu_1_1' : 1e-2,\n",
    "    #'relu_4_2' : 5e-3,\n",
    "    'relu_4_2' : 2e-2,\n",
    "    #'relu_4_2' : 1e0,\n",
    "}\n",
    "\n",
    "STYLE_LOSS_WEIGHTS = {\n",
    "    'relu_1_1' : 1e3,\n",
    "    'relu_2_1' : 5e3, # 5e3\n",
    "    'relu_3_1' : 1e3,\n",
    "    'relu_4_1' : 1e3, # 1e3\n",
    "    'relu_5_1' : 1e3,\n",
    "}\n",
    "\n",
    "STYLE_LOSS_ALPHA = 1.0\n",
    "KLD_LOSS_WEIGHT = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "TRAINING_PORTION_STYLE=128 # 128\n",
    "\n",
    "data_style_train = data.load_dataset(\"../dataset/style_cherrypicked/train\", resolution=RESOLUTION)\n",
    "data_style_test = data.load_dataset(\"../dataset/style_cherrypicked/test\", resolution=RESOLUTION)\n",
    "data_loader_style_train = DataLoader(data_style_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "data_loader_style_test = DataLoader(data_style_test, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "data_content_test = data.load_debug_dataset('../dataset/content_test', resolution=RESOLUTION)\n",
    "data_loader_content_test = DataLoader(data_content_test, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "data_loader_test_overfit = data.DatasetPairIterator(data_loader_content_test, data_loader_style_train)\n",
    "data_loader_test = data.DatasetPairIterator(data_loader_content_test, data_style_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNUP_CONVOLUTIONS = 5 #3\n",
    "ADAIN_CONVOLUTIONS = 7 #3\n",
    "STYLE_DOWN_CONVOLUTIONS = 5 #3\n",
    "NUM_LAYERS_NO_CONNECTION = 0\n",
    "RESIDUAL_STYLE = True # False\n",
    "RESIDUAL_DOWN = True # False\n",
    "RESIDUAL_ADAIN = True\n",
    "RESIDUAL_UP = True\n",
    "STYLE_NORM = True\n",
    "DOWN_NORM = 'in'\n",
    "UP_NORM = 'adain'\n",
    "\n",
    "if not VAE:\n",
    "    STYLE_DIM = STYLE_DIM * 2\n",
    "\n",
    "#normalization = ['adain', 'adain', None, None, None]\n",
    "unet = model.unet.UNetAutoencoder(3, STYLE_DIM, residual_downsampling=RESIDUAL_DOWN, residual_adain=RESIDUAL_ADAIN, residual_upsampling=RESIDUAL_UP, \n",
    "        down_normalization=DOWN_NORM, up_normalization=UP_NORM, num_adain_convolutions=ADAIN_CONVOLUTIONS, \n",
    "        num_downup_convolutions=DOWNUP_CONVOLUTIONS, num_downup_without_connections=NUM_LAYERS_NO_CONNECTION, output_activation='sigmoid')\n",
    "\n",
    "if VAE:\n",
    "    style_encoder = model.autoencoder.Encoder(2 * STYLE_DIM, normalization=STYLE_NORM, residual=RESIDUAL_STYLE, num_down_convolutions=STYLE_DOWN_CONVOLUTIONS)\n",
    "else:\n",
    "    style_encoder = model.autoencoder.Encoder(STYLE_DIM, normalization=STYLE_NORM, residual=RESIDUAL_STYLE, num_down_convolutions=STYLE_DOWN_CONVOLUTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model\n",
    "state_dict = torch.load('./models/model.pt')\n",
    "unet.load_state_dict(state_dict['unet_state_dict'])\n",
    "style_encoder.load_state_dict(state_dict['style_encoder_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): \n",
    "    unet = unet.cuda()\n",
    "    style_encoder = style_encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(content_image, style_image):\n",
    "    \"\"\" Forward pass through the architecture.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    content_image : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The content images.\n",
    "    style_image : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The style images.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    stylized : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The stylizations.\n",
    "    style_encoding : torch.Tensor, shape [batch_size, STYLE_DIM]\n",
    "        Style encodings.\n",
    "    style_encoding_mean : torch.Tensor, shape [batch_size, STYLE_DIM]\n",
    "        Means for the style encodings.\n",
    "    style_encoding_logvar : torch.Tensor, shape [batch_size, STYLE_DIM]\n",
    "        Logarithm of the variances of the style encodings.\n",
    "    \"\"\"\n",
    "    style_stats = style_encoder(style_image)\n",
    "    style_mean = style_stats[..., : STYLE_DIM]\n",
    "    style_logvar = style_stats[..., STYLE_DIM : ]\n",
    "    style_sample = function.sample_normal(style_mean, style_logvar)\n",
    "    stylized = unet(content_image, style_sample)\n",
    "    return stylized, style_sample, style_mean, style_logvar\n",
    "\n",
    "def forward_no_vae(content_image, style_image):\n",
    "    \"\"\" Forward pass through the architecture.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    content_image : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The content images.\n",
    "    style_image : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The style images.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    stylized : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The stylizations.\n",
    "    style_encoding : torch.Tensor, shape [batch_size, STYLE_DIM]\n",
    "        Style encodings.\n",
    "    \"\"\"\n",
    "    style_encoding = style_encoder(style_image)\n",
    "    stylized = unet(content_image, style_encoding)\n",
    "    return stylized, style_encoding\n",
    "\n",
    "def forward_interpolate(content_image, style_image1, style_image2, interpolation_factor):\n",
    "    \"\"\" Forward pass through the architecture.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    content_image : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The content images.\n",
    "    style_image : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The style images.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    stylized : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The stylizations.\n",
    "    style_encoding : torch.Tensor, shape [batch_size, STYLE_DIM]\n",
    "        Style encodings.\n",
    "    style_encoding_mean : torch.Tensor, shape [batch_size, STYLE_DIM]\n",
    "        Means for the style encodings.\n",
    "    style_encoding_logvar : torch.Tensor, shape [batch_size, STYLE_DIM]\n",
    "        Logarithm of the variances of the style encodings.\n",
    "    \"\"\"\n",
    "    style_stats1 = style_encoder(style_image1)\n",
    "    style_mean1 = style_stats1[..., : STYLE_DIM]\n",
    "    style_logvar1 = style_stats1[..., STYLE_DIM : ]\n",
    "    style_sample1 = function.sample_normal(style_mean1, style_logvar1)\n",
    "    \n",
    "    style_stats2 = style_encoder(style_image2)\n",
    "    style_mean2 = style_stats2[..., : STYLE_DIM]\n",
    "    style_logvar2 = style_stats2[..., STYLE_DIM : ]\n",
    "    style_sample2 = function.sample_normal(style_mean2, style_logvar2)\n",
    "    \n",
    "    style_sample = interpolation_factor * style_sample1 + (1 - interpolation_factor) * style_sample2\n",
    "    \n",
    "    stylized = unet(content_image, style_sample)\n",
    "    return stylized, style_sample\n",
    "\n",
    "def forward_sample(content_image):\n",
    "    \"\"\" Forward pass through the architecture.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    content_image : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The content images.\n",
    "    style_image : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The style images.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    stylized : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The stylizations.\n",
    "    style_encoding : torch.Tensor, shape [batch_size, STYLE_DIM]\n",
    "        Style encodings.\n",
    "    style_encoding_mean : torch.Tensor, shape [batch_size, STYLE_DIM]\n",
    "        Means for the style encodings.\n",
    "    style_encoding_logvar : torch.Tensor, shape [batch_size, STYLE_DIM]\n",
    "        Logarithm of the variances of the style encodings.\n",
    "    \"\"\"\n",
    "    style_sample = torch.randn((BATCH_SIZE, STYLE_DIM), device=content_image.device, requires_grad=False)\n",
    "    stylized = unet(content_image, style_sample)\n",
    "    return stylized, style_sample\n",
    "\n",
    "def forward_interpolate_no_vae(content_image, style_image1, style_image2, interpolation_factor):\n",
    "    \"\"\" Forward pass through the architecture.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    content_image : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The content images.\n",
    "    style_image : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The style images.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    stylized : torch.Tensor, shape [batch_size, 3, H, W]\n",
    "        The stylizations.\n",
    "    style_encoding : torch.Tensor, shape [batch_size, STYLE_DIM]\n",
    "        Style encodings.\n",
    "    style_encoding_mean : torch.Tensor, shape [batch_size, STYLE_DIM]\n",
    "        Means for the style encodings.\n",
    "    style_encoding_logvar : torch.Tensor, shape [batch_size, STYLE_DIM]\n",
    "        Logarithm of the variances of the style encodings.\n",
    "    \"\"\"\n",
    "    style_embedding1 = style_encoder(style_image1)\n",
    "    style_embedding2 = style_encoder(style_image2)\n",
    "    \n",
    "    style_embedding = interpolation_factor * style_embedding1 + (1 - interpolation_factor) * style_embedding2\n",
    "    \n",
    "    stylized = unet(content_image, style_embedding)\n",
    "    return stylized, style_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pertrub styles (training set)\n",
    "We perturb styles by projecting them using PCA, adjusting the component that captures most of the variance and projecting back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PERTURBATIONS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed all training styles\n",
    "style_embeddings = np.empty((len(data_style_train), STYLE_DIM))\n",
    "style_encoder.eval()\n",
    "num_embedded = 0\n",
    "with torch.no_grad():\n",
    "    for styles, _ in data_loader_style_train:\n",
    "        for idx in range(styles.size(0)):\n",
    "            embedding_stats = style_encoder(styles[idx : idx + 1].cuda())\n",
    "            embedding_mean = embedding_stats[0, : STYLE_DIM]\n",
    "            style_embeddings[num_embedded] = embedding_mean.detach().cpu().numpy()\n",
    "            num_embedded += 1\n",
    "style_embeddings = style_embeddings[:num_embedded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9917115380511897 of total variance is captured by pca\n",
      "0.19497261764524146 of total variance is captured by first component\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=64)\n",
    "embeddings_transformed = pca.fit_transform(style_embeddings)\n",
    "print(f'{pca.explained_variance_ratio_.sum()} of total variance is captured by pca')\n",
    "print(f'{pca.explained_variance_ratio_[0]} of total variance is captured by first component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_encoder.cpu() # Save my poor GPU memory...\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 96, 512)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilons = np.linspace(-10, 10, NUM_PERTURBATIONS)\n",
    "s = np.tile(embeddings_transformed, (epsilons.shape[0], 1, 1))\n",
    "s[:, :, 0] += epsilons.reshape((-1, 1))\n",
    "embeddings_perturbed = pca.inverse_transform(s.reshape((-1, s.shape[-1])))\n",
    "embeddings_perturbed = embeddings_perturbed.reshape((s.shape[0], s.shape[1], -1))\n",
    "embeddings_perturbed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERTURBATION_ITERATIONS = embeddings_perturbed.shape[1]\n",
    "with torch.no_grad():\n",
    "    val_iteration = 0\n",
    "    unet.eval()\n",
    "    for (content_image, content_path), _ in data_loader_val:\n",
    "\n",
    "        if val_iteration >= PERTURBATION_ITERATIONS:\n",
    "            break\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            content_image = content_image.to('cuda')\n",
    "        style_encoding = torch.from_numpy(embeddings_perturbed[:, val_iteration, :]).to('cuda').float()\n",
    "        content_image = content_image[:style_encoding.size(0)]\n",
    "        stylized = unet(content_image, style_encoding)\n",
    "        \n",
    "            \n",
    "        tb_writer.add_images('perturbations along first pc', torch.from_numpy(np.concatenate([\n",
    "            img.detach().cpu().numpy() for img in [content_image[0:1], stylized]\n",
    "        ])), val_iteration)\n",
    "        val_iteration += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturb along two principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33270730367451795 of total variance is captured by first two components\n"
     ]
    }
   ],
   "source": [
    "print(f'{pca.explained_variance_ratio_[:2].sum()} of total variance is captured by first two components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.linspace(-10, 10, NUM_PERTURBATIONS), np.linspace(-10, 10, NUM_PERTURBATIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5, 96, 512)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = np.tile(embeddings_transformed, (NUM_PERTURBATIONS, NUM_PERTURBATIONS, 1, 1))\n",
    "s[:, :, :, 0] += xx.reshape((NUM_PERTURBATIONS, NUM_PERTURBATIONS, 1))\n",
    "s[:, :, :, 1] += yy.reshape((NUM_PERTURBATIONS, NUM_PERTURBATIONS, 1))\n",
    "embeddings_perturbed = pca.inverse_transform(s.reshape((-1, s.shape[-1])))\n",
    "embeddings_perturbed = embeddings_perturbed.reshape(list(s.shape[:-1]) +  [-1])\n",
    "embeddings_perturbed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "PERTURBATION_ITERATIONS = embeddings_perturbed.shape[-2]\n",
    "with torch.no_grad():\n",
    "    val_iteration = 0\n",
    "    unet.eval()\n",
    "    unet.cpu()\n",
    "    \n",
    "    for (content_image, content_path), _ in data_loader_val:\n",
    "\n",
    "        if val_iteration >= PERTURBATION_ITERATIONS:\n",
    "            break\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            content_image = content_image\n",
    "        style_encoding = torch.from_numpy(embeddings_perturbed[:, :, val_iteration, :]).float()\n",
    "        # Flatten to input as NUM_PERTURBATIONS x NUM_PERTURBATIONS batch\n",
    "        style_encoding = style_encoding.view(NUM_PERTURBATIONS * NUM_PERTURBATIONS, style_encoding.size(-1))\n",
    "        \n",
    "        content_image = content_image[0].unsqueeze(0).expand((style_encoding.size(0), -1, -1, -1))\n",
    "        \n",
    "        stylized = unet(content_image, style_encoding)\n",
    "        tb_writer.add_image('perturbations along first two pcs', \n",
    "                            torchvision.utils.make_grid(stylized, nrow=NUM_PERTURBATIONS), val_iteration)\n",
    "        print(val_iteration)\n",
    "        val_iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
